{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas iniciales con RLLIB y RAY\n",
    "### Javier Guzmán Muñoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#imports necesarios\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "import json, os, shutil, sys\n",
    "import gym\n",
    "import pprint\n",
    "import time\n",
    "import shelve\n",
    "from tensorflow import keras\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al inicializar ray podemos configurar varios parámetros.\n",
    "Entre ellos:\n",
    "- `local_mode=True` para no distribuir el trabajo entre workers paralelos.\n",
    "- `_metrics_export_port` el puerto donde se van a exportar las métricas que podemos ver en el Dashboard. Las podemos visualizar y graficar con prometheus https://docs.ray.io/en/master/ray-metrics.html\n",
    "- `num_cpus`, `num_gpus` para establecer los recursos sobre los que queremos trabajar. En este caso, tenemos 4 CPUs (los 4 cores de mi laptop) y ninguna GPU.\n",
    "-`ignore_reinit_error=True` para que no nos de error al hacer `ray.init` si ray ya estaba inicializado. En ese caso no se vuelve a inicializar ray.\n",
    "\n",
    "Enlace a la documentación de etsa función: https://docs.ray.io/en/master/package-ref.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 18:24:26,266\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.10.1.128',\n",
       " 'raylet_ip_address': '10.10.1.128',\n",
       " 'redis_address': '10.10.1.128:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-11-19_18-24-24_611567_209/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-11-19_18-24-24_611567_209/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-11-19_18-24-24_611567_209',\n",
       " 'metrics_export_port': 63419,\n",
       " 'node_id': '31cd14c23ee3ba5c053c3dc874859cdf1a79b7c5'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inicializamos ray\n",
    "ray.shutdown()\n",
    "\n",
    "# Si no ponemos un puerto para exportar las métricas se pone uno aleatorio.\n",
    "ray.init(ignore_reinit_error=True, _metrics_export_port=63419)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso Básico: Problema basado en texto: Taxi\n",
    "Ejemplo de https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración del agente:\n",
      "\n",
      "{'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 4000, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, '_time_major': False, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {}, 'env': None, 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 5e-05, 'monitor': False, 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, '_use_trajectory_view_api': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent'}, 'logger_config': None, 'replay_sequence_length': 1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_share_layers': False, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'simple_optimizer': False, '_fake_gpus': False}\n",
      "\n",
      "Configuración del modelo del agente:\n",
      "\n",
      "{'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, '_time_major': False, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None}\n"
     ]
    }
   ],
   "source": [
    "#Directorio donde guardaremos checkpoints\n",
    "CHECKPOINT_ROOT = \"/tmp/ppo/taxi\"\n",
    "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
    "\n",
    "#Directorio donde se guardan los resultados de cada sesión de ray (los podemos ver con tensorboard)\n",
    "ray_results =os.getenv(\"HOME\")+ \"/ray_results\"\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)\n",
    "\n",
    "#Entorno: problema del taxi (https://gym.openai.com/envs/Taxi-v3/)\n",
    "SELECT_ENV = \"Taxi-v3\"\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "print(\"Configuración del agente:\\n\\n\" + str(config))\n",
    "print(\"\\nConfiguración del modelo del agente:\\n\\n\" + str(config[\"model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1466)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=1466)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=1466)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=1468)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=1468)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=1468)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-19 18:25:40,306\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "#Configuración del agente\n",
    "agent = ppo.PPOTrainer(config, env=SELECT_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una ve creado el agente, cada etapa de su entrenamiento genera unos resultados entre los que podemos estraerinformación muy útil, como estaísiticos sobre las recompensas, la longitud de los episodios o distintas mediciones de tiempos. En la siguiente celda se puede ver un ejemplo de ello para el agente del taxi, donde vemos los valores de los timers, que nos permiten ver, entre otros, el tiempo de aprendizaje de cada iteración de train(), por ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.train()\n",
    "pprint.pprint(result)\n",
    "\n",
    "#Observar los valores de timers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1468)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=1468)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=1468)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=1466)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=1466)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=1466)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward -911.00/-768.45/-391.00 len 196.20 learn_time(ms) 4551.36 saved /tmp/ppo/taxi/checkpoint_1/checkpoint-1\n",
      "  2 reward -911.00/-725.29/-99.00 len 192.17 learn_time(ms) 4207.62 saved /tmp/ppo/taxi/checkpoint_2/checkpoint-2\n",
      "  3 reward -911.00/-689.68/-99.00 len 191.63 learn_time(ms) 3973.75 saved /tmp/ppo/taxi/checkpoint_3/checkpoint-3\n",
      "  4 reward -911.00/-655.88/-99.00 len 191.24 learn_time(ms) 3852.65 saved /tmp/ppo/taxi/checkpoint_4/checkpoint-4\n",
      "  5 reward -911.00/-621.44/-13.00 len 189.41 learn_time(ms) 3780.21 saved /tmp/ppo/taxi/checkpoint_5/checkpoint-5\n",
      "  6 reward -857.00/-556.93/-13.00 len 185.26 learn_time(ms) 3795.93 saved /tmp/ppo/taxi/checkpoint_6/checkpoint-6\n",
      "  7 reward -794.00/-470.82/-13.00 len 172.53 learn_time(ms) 3798.64 saved /tmp/ppo/taxi/checkpoint_7/checkpoint-7\n",
      "  8 reward -794.00/-435.44/-13.00 len 168.53 learn_time(ms) 3799.52 saved /tmp/ppo/taxi/checkpoint_8/checkpoint-8\n",
      "  9 reward -686.00/-397.51/-37.00 len 163.03 learn_time(ms) 3801.69 saved /tmp/ppo/taxi/checkpoint_9/checkpoint-9\n",
      " 10 reward -974.00/-379.04/-11.00 len 161.36 learn_time(ms) 3803.85 saved /tmp/ppo/taxi/checkpoint_10/checkpoint-10\n",
      " 11 reward -974.00/-365.10/-11.00 len 163.32 learn_time(ms) 3731.68 saved /tmp/ppo/taxi/checkpoint_11/checkpoint-11\n",
      " 12 reward -974.00/-341.22/ -5.00 len 158.94 learn_time(ms) 3741.97 saved /tmp/ppo/taxi/checkpoint_12/checkpoint-12\n",
      " 13 reward -974.00/-333.08/ -5.00 len 157.91 learn_time(ms) 3780.61 saved /tmp/ppo/taxi/checkpoint_13/checkpoint-13\n",
      " 14 reward -974.00/-308.00/  1.00 len 153.86 learn_time(ms) 3812.77 saved /tmp/ppo/taxi/checkpoint_14/checkpoint-14\n",
      " 15 reward -857.00/-295.40/  1.00 len 151.79 learn_time(ms) 3844.45 saved /tmp/ppo/taxi/checkpoint_15/checkpoint-15\n",
      " 16 reward -569.00/-286.63/ 12.00 len 154.42 learn_time(ms) 3843.43 saved /tmp/ppo/taxi/checkpoint_16/checkpoint-16\n",
      " 17 reward -569.00/-255.56/ 12.00 len 148.55 learn_time(ms) 3841.49 saved /tmp/ppo/taxi/checkpoint_17/checkpoint-17\n",
      " 18 reward -542.00/-233.94/ 12.00 len 143.16 learn_time(ms) 3842.76 saved /tmp/ppo/taxi/checkpoint_18/checkpoint-18\n",
      " 19 reward -542.00/-224.27/ 12.00 len 140.63 learn_time(ms) 3844.46 saved /tmp/ppo/taxi/checkpoint_19/checkpoint-19\n",
      " 20 reward -515.00/-209.12/  7.00 len 138.68 learn_time(ms) 3843.29 saved /tmp/ppo/taxi/checkpoint_20/checkpoint-20\n",
      " 21 reward -515.00/-210.63/  7.00 len 143.61 learn_time(ms) 3843.64 saved /tmp/ppo/taxi/checkpoint_21/checkpoint-21\n",
      " 22 reward -515.00/-183.81/  7.00 len 134.31 learn_time(ms) 3851.70 saved /tmp/ppo/taxi/checkpoint_22/checkpoint-22\n",
      " 23 reward -488.00/-168.21/  7.00 len 130.83 learn_time(ms) 3855.28 saved /tmp/ppo/taxi/checkpoint_23/checkpoint-23\n",
      " 24 reward -488.00/-163.95/  7.00 len 129.12 learn_time(ms) 3861.66 saved /tmp/ppo/taxi/checkpoint_24/checkpoint-24\n",
      " 25 reward -353.00/-162.85/  5.00 len 134.35 learn_time(ms) 3875.47 saved /tmp/ppo/taxi/checkpoint_25/checkpoint-25\n",
      " 26 reward -362.00/-159.51/  5.00 len 133.26 learn_time(ms) 3892.45 saved /tmp/ppo/taxi/checkpoint_26/checkpoint-26\n",
      " 27 reward -362.00/-146.21/  3.00 len 127.31 learn_time(ms) 3928.56 saved /tmp/ppo/taxi/checkpoint_27/checkpoint-27\n",
      " 28 reward -371.00/-120.40/  9.00 len 109.18 learn_time(ms) 3954.59 saved /tmp/ppo/taxi/checkpoint_28/checkpoint-28\n",
      " 29 reward -371.00/-108.98/ 10.00 len 103.70 learn_time(ms) 3974.97 saved /tmp/ppo/taxi/checkpoint_29/checkpoint-29\n",
      " 30 reward -371.00/-107.70/ 10.00 len 103.89 learn_time(ms) 3980.84 saved /tmp/ppo/taxi/checkpoint_30/checkpoint-30\n",
      "Total learn time: 116511.27000000002\n",
      "Average learn time per iteration: 3883.7090000000007\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos el modelo con 30 iteraciones, llamando al método train sobre agent\n",
    "N_ITER = 30\n",
    "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} learn_time(ms) {:6.2f} saved {}\"\n",
    "\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "total_learn_time = 0\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    episode = {'n': n,\n",
    "               'episode_reward_min': result['episode_reward_min'],\n",
    "               'episode_reward_mean': result['episode_reward_mean'],\n",
    "               'episode_reward_max': result['episode_reward_max'],\n",
    "               'episode_len_mean': result['episode_len_mean'],\n",
    "               'learn_time_ms': result['timers']['learn_time_ms']}\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(CHECKPOINT_ROOT)\n",
    "    print(s.format(\n",
    "    n + 1,\n",
    "    result[\"episode_reward_min\"],\n",
    "    result[\"episode_reward_mean\"],\n",
    "    result[\"episode_reward_max\"],\n",
    "    result[\"episode_len_mean\"],\n",
    "    result[\"timers\"][\"learn_time_ms\"],\n",
    "    file_name\n",
    "   ))\n",
    "    total_learn_time+= result[\"timers\"][\"learn_time_ms\"]\n",
    "    \n",
    "print(\"Total learn time: \" + str(total_learn_time))\n",
    "print(\"Average learn time per iteration: \" + str(total_learn_time/N_ITER))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          128256      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          128256      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 6)            1542        fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 389,895\n",
      "Trainable params: 389,895\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Variables\n",
      "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(500, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(500, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(256, 6) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/bias:0' shape=(6,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/kernel:0' shape=(256, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
      "Value function\n",
      "<tf.Tensor 'Reshape_4:0' shape=(?,) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "#Visualizamos algunos datos del modelo entrenado\n",
    "\n",
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())\n",
    "keras.utils.plot_model(model.base_model, \"taxi_model.png\", show_shapes=True)\n",
    "\n",
    "print(\"Variables\")\n",
    "pprint.pprint(model.variables())\n",
    "print(\"Value function\")\n",
    "pprint.pprint(model.value_function())\n",
    " \n",
    "#Linea de tiempo que se puede abrir en chrome://tracing y se ve el reparto de tareas y las tareas ejecutasdas por los workers\n",
    "# así como medir tiempos\n",
    "ray.timeline(\"/mnt/c/Users/javig/timelines/time_taxi.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos entrenado un modelo con una capa input con una dimensión concreta, en este caso de tamaño 50, por lo que para probar el modelo necesitamos un entorno que nos proporcione entradas de estas características. Al ser entornos tan concretos no tenemos ninguno que reúna estos requisitos, por lo que lo vamos a probar con el mismo prolema con el que le hemos entrenado.\n",
    "\n",
    "Ejecutamos un rollout de 10 episodios (esto es, tomamos acciones en los modelos hasta llegaral estado de Done=True 10 veces). Cada episodio tiene una longitud arbitraria. Guardamos toda la info posible en el fichero taxi.pkl.\n",
    "Medimos el tiempo del rollout en conjunto pues al ser un comando predefinido (en verdad estamos ejecutando el script rollout.py) de momento no vamos a hacer más. Más adelante veremos como precisar esto un poco más. este tiempo es bastante improciso pues cuenta también el tiempo de inicializar ray o escribir los datos, entre otros\n",
    "\n",
    "El problema del taxi es un entorno denominado 'ToyText', es decir que su salida es únicamente \"un texto\". Sus estados se codifican como un entero del 0 al 499 y sus acciones con otro entero del 0 al 5 (moverse en las 4 direcciones, dejar o coger un pasajero).\n",
    "\n",
    "Las recompensas son -1 en cualquier acción salvo si es un intento de coger o soltar al pasajero ilegalmente (porque el pasajero no está donde está el taxi o porque lo intenta soltra cuando no lleva pasajero o en un destino al que no quiere ir) y 20 si sulta al pasajero correctamente (en su posición)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time.time()\n",
    "!rllib rollout /tmp/ppo/taxi/checkpoint_30/checkpoint-30 --run PPO --env=Taxi-v3 --episodes 10  --out 'taxi.pkl' --save-info --use-shelve --track-progress --video-dir /mnt/c/Users/javig/videoGym\n",
    "t1=time.time()-t0\n",
    "print(\"El rollout ha tardado {} segundos\".format(t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El fichero de salida está en formato shelve pickle y con un shleve para cada rollout (para cada episodio). En el propio fichero\n",
    "fuente de rollout.py se nos indica como procesar esta infromación.\n",
    "\n",
    "Lo que obtenemos es, para cada episodio, el número del mismo y una lista con tantos elementos (que son a su vez listas) como pasos hayamos dado en ese episodio con iformación acerca de los mismos. Esta información es:\n",
    "- `obs`: estado observado antes de tomar la acción.\n",
    "- `action`: acción tomada en base al estado observado, la última acción tomada y la última recompensa obtenida\n",
    "- `next_obs`: el estado observado tras tomar la acción y que será el estado incial del siguiente elemento de la lista\n",
    "- `done`: booleano que inidca si estamos en un estado 'Done': esto es si hemos conseguido el objetivo del juego o si hemos agotado algún temporizador.\n",
    "-`info`: este campo sólo se incluye si hemos activado el flag `--save-info`e incluye información sobre el entorno y el problema concreto. En este caso, cuando hemos llegado a un estado 'Done' por agotra un temporizador se nos informa en este diccionario de ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El fichero está en formato shelve pickle y con un shleve para cada rollout (para cada episodio). En el propio fichero\n",
    "# fuente de rollout.py se nos indica como procesar esta infromación\n",
    "with shelve.open('taxi.pkl') as rollouts:\n",
    "    for episode_index in range(rollouts[\"num_episodes\"]):\n",
    "        rollout = rollouts[str(episode_index)]\n",
    "        print(str(episode_index))\n",
    "        pprint.pprint(rollout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función genérica para el entrenamiento.\n",
    "A continuación, basada en el ejemplo anterior, esta función ejecuta iteraciones de entrenamiento sobre un entorno indicado guardando checkpoints en una ruta especificada y con la configuración elegida por nosotros también. Los parámetros que recibe son:\n",
    "- `checkpoint_root`: directorio en el que queremos guardar nuestros checkpoints (uno por cada iteración)\n",
    "- `env`: entorno gym con el que crearemos el agente\n",
    "- `config`: configuración para nuestro agente: aquí podemos indicar las capas del modelo\n",
    "- `n_iter`: número de iteraciones a realizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_train(checkpoint_root, env, config, n_iter):\n",
    "    shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)\n",
    "    agent = ppo.PPOTrainer(config=config, env=env)\n",
    "    s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} learn_time(ms) {:6.2f} saved {}\"\n",
    "    \n",
    "    \n",
    "\n",
    "    results = []\n",
    "    episode_data = []\n",
    "    episode_json = []\n",
    "\n",
    "    total_learn_time = 0\n",
    "    for n in range(n_iter):\n",
    "        result = agent.train()\n",
    "        results.append(result)\n",
    "        episode = {'n': n,\n",
    "                   'episode_reward_min': result['episode_reward_min'],\n",
    "                   'episode_reward_mean': result['episode_reward_mean'],\n",
    "                   'episode_reward_max': result['episode_reward_max'],\n",
    "                   'episode_len_mean': result['episode_len_mean'],\n",
    "                   'learn_time_ms': result['timers']['learn_time_ms']}\n",
    "        episode_data.append(episode)\n",
    "        episode_json.append(json.dumps(episode))\n",
    "        file_name = agent.save(checkpoint_root)\n",
    "        print(s.format(\n",
    "        n + 1,\n",
    "        result[\"episode_reward_min\"],\n",
    "        result[\"episode_reward_mean\"],\n",
    "        result[\"episode_reward_max\"],\n",
    "        result[\"episode_len_mean\"],\n",
    "        result[\"timers\"][\"learn_time_ms\"],\n",
    "        file_name\n",
    "       ))\n",
    "        total_learn_time+= result[\"timers\"][\"learn_time_ms\"]\n",
    "\n",
    "    print(\"Total learn time: \" + str(total_learn_time))\n",
    "    print(\"Average learn time per iteration: \" + str(total_learn_time/n_iter))\n",
    "    policy = agent.get_policy()\n",
    "    model = policy.model\n",
    "    print(model.base_model.summary())\n",
    "\n",
    "    print(\"Variables\")\n",
    "    pprint.pprint(model.variables())\n",
    "    print(\"Value function\")\n",
    "    pprint.pprint(model.value_function())\n",
    "    keras.utils.plot_model(model.base_model, \"taxi_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo con dos capas internas de 512 neuronas cada una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 19:18:23,380\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=2954)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=2954)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=2954)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=2960)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=2960)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=2960)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-19 19:18:34,030\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=2954)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=2954)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=2954)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=2960)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=2960)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=2960)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward -929.00/-775.55/-303.00 len 193.55 learn_time(ms) 7758.57 saved /tmp/ppo/taxi_2/checkpoint_1/checkpoint-1\n",
      "  2 reward -929.00/-777.58/-303.00 len 196.78 learn_time(ms) 7717.17 saved /tmp/ppo/taxi_2/checkpoint_2/checkpoint-2\n",
      "  3 reward -929.00/-757.42/-303.00 len 197.32 learn_time(ms) 8252.94 saved /tmp/ppo/taxi_2/checkpoint_3/checkpoint-3\n",
      "  4 reward -929.00/-728.15/-233.00 len 194.93 learn_time(ms) 8431.28 saved /tmp/ppo/taxi_2/checkpoint_4/checkpoint-4\n",
      "  5 reward -929.00/-713.07/-233.00 len 194.79 learn_time(ms) 8523.70 saved /tmp/ppo/taxi_2/checkpoint_5/checkpoint-5\n",
      "  6 reward -911.00/-685.78/-233.00 len 194.80 learn_time(ms) 8526.81 saved /tmp/ppo/taxi_2/checkpoint_6/checkpoint-6\n",
      "  7 reward -911.00/-656.82/-233.00 len 194.76 learn_time(ms) 8577.41 saved /tmp/ppo/taxi_2/checkpoint_7/checkpoint-7\n",
      "  8 reward -911.00/-628.68/-203.00 len 192.06 learn_time(ms) 8595.07 saved /tmp/ppo/taxi_2/checkpoint_8/checkpoint-8\n",
      "  9 reward -1028.00/-613.69/-76.00 len 191.68 learn_time(ms) 8654.15 saved /tmp/ppo/taxi_2/checkpoint_9/checkpoint-9\n",
      " 10 reward -1028.00/-595.19/-76.00 len 190.85 learn_time(ms) 8667.27 saved /tmp/ppo/taxi_2/checkpoint_10/checkpoint-10\n",
      " 11 reward -1028.00/-571.69/-76.00 len 188.83 learn_time(ms) 8793.21 saved /tmp/ppo/taxi_2/checkpoint_11/checkpoint-11\n",
      " 12 reward -1028.00/-532.49/ -3.00 len 180.59 learn_time(ms) 8897.73 saved /tmp/ppo/taxi_2/checkpoint_12/checkpoint-12\n",
      " 13 reward -884.00/-511.16/ -3.00 len 180.17 learn_time(ms) 8854.55 saved /tmp/ppo/taxi_2/checkpoint_13/checkpoint-13\n",
      " 14 reward -884.00/-487.00/ -3.00 len 177.55 learn_time(ms) 8848.36 saved /tmp/ppo/taxi_2/checkpoint_14/checkpoint-14\n",
      " 15 reward -938.00/-448.13/ -3.00 len 171.29 learn_time(ms) 8806.68 saved /tmp/ppo/taxi_2/checkpoint_15/checkpoint-15\n",
      " 16 reward -938.00/-449.94/ -4.00 len 175.83 learn_time(ms) 8802.78 saved /tmp/ppo/taxi_2/checkpoint_16/checkpoint-16\n",
      " 17 reward -938.00/-425.41/ -4.00 len 168.61 learn_time(ms) 8793.30 saved /tmp/ppo/taxi_2/checkpoint_17/checkpoint-17\n",
      " 18 reward -938.00/-389.16/ -4.00 len 162.96 learn_time(ms) 8837.73 saved /tmp/ppo/taxi_2/checkpoint_18/checkpoint-18\n",
      " 19 reward -929.00/-337.78/  4.00 len 150.76 learn_time(ms) 9024.90 saved /tmp/ppo/taxi_2/checkpoint_19/checkpoint-19\n",
      " 20 reward -929.00/-296.50/  4.00 len 143.14 learn_time(ms) 9091.19 saved /tmp/ppo/taxi_2/checkpoint_20/checkpoint-20\n",
      " 21 reward -632.00/-221.63/  7.00 len 117.74 learn_time(ms) 9113.76 saved /tmp/ppo/taxi_2/checkpoint_21/checkpoint-21\n",
      " 22 reward -632.00/-204.39/  7.00 len 111.30 learn_time(ms) 9154.55 saved /tmp/ppo/taxi_2/checkpoint_22/checkpoint-22\n",
      " 23 reward -632.00/-197.30/  7.00 len 109.85 learn_time(ms) 9147.15 saved /tmp/ppo/taxi_2/checkpoint_23/checkpoint-23\n",
      " 24 reward -596.00/-190.85/ 11.00 len 111.92 learn_time(ms) 9166.18 saved /tmp/ppo/taxi_2/checkpoint_24/checkpoint-24\n",
      " 25 reward -596.00/-160.33/ 11.00 len 101.65 learn_time(ms) 9287.34 saved /tmp/ppo/taxi_2/checkpoint_25/checkpoint-25\n",
      " 26 reward -470.00/-138.47/  6.00 len  95.33 learn_time(ms) 9377.69 saved /tmp/ppo/taxi_2/checkpoint_26/checkpoint-26\n",
      " 27 reward -407.00/-140.37/ 13.00 len  96.90 learn_time(ms) 9399.46 saved /tmp/ppo/taxi_2/checkpoint_27/checkpoint-27\n",
      " 28 reward -443.00/-145.86/ 13.00 len 100.11 learn_time(ms) 9444.03 saved /tmp/ppo/taxi_2/checkpoint_28/checkpoint-28\n",
      " 29 reward -443.00/-118.47/ 13.00 len  89.55 learn_time(ms) 9195.49 saved /tmp/ppo/taxi_2/checkpoint_29/checkpoint-29\n",
      " 30 reward -362.00/-96.27/  9.00 len  79.20 learn_time(ms) 9171.20 saved /tmp/ppo/taxi_2/checkpoint_30/checkpoint-30\n",
      "Total learn time: 264911.641\n",
      "Average learn time per iteration: 8830.388033333333\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 512)          256512      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 512)          256512      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 512)          262656      fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 512)          262656      fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 6)            3078        fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            513         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,041,927\n",
      "Trainable params: 1,041,927\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Variables\n",
      "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(500, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_1/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(500, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(512, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(512, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(512, 6) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/bias:0' shape=(6,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/kernel:0' shape=(512, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
      "Value function\n",
      "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _metrics_export_port=63419)\n",
    "checkpoint_root = '/tmp/ppo/taxi_2'\n",
    "env = 'Taxi-v3'\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['model']['fcnet_hiddens'] = [512,512]\n",
    "n_iter = 30\n",
    "full_train(checkpoint_root, env, config, n_iter)\n",
    "ray.timeline(\"/mnt/c/Users/javig/timelines/time_taxi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo con dos capas internas de 1024 neurnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 19:25:17,303\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=3130)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3130)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3130)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=3132)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3132)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3132)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-19 19:25:28,224\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=3130)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3130)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3130)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=3132)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3132)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3132)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward -830.00/-755.70/-469.00 len 197.40 learn_time(ms) 25632.94 saved /tmp/ppo/taxi_3/checkpoint_1/checkpoint-1\n",
      "  2 reward -866.00/-764.77/-469.00 len 198.38 learn_time(ms) 25892.08 saved /tmp/ppo/taxi_3/checkpoint_2/checkpoint-2\n",
      "  3 reward -965.00/-765.42/-464.00 len 197.67 learn_time(ms) 25895.62 saved /tmp/ppo/taxi_3/checkpoint_3/checkpoint-3\n",
      "  4 reward -965.00/-754.50/-455.00 len 197.62 learn_time(ms) 25926.15 saved /tmp/ppo/taxi_3/checkpoint_4/checkpoint-4\n",
      "  5 reward -965.00/-744.88/-212.00 len 195.58 learn_time(ms) 26048.74 saved /tmp/ppo/taxi_3/checkpoint_5/checkpoint-5\n",
      "  6 reward -965.00/-732.06/-212.00 len 193.32 learn_time(ms) 26050.15 saved /tmp/ppo/taxi_3/checkpoint_6/checkpoint-6\n",
      "  7 reward -1010.00/-701.97/-212.00 len 191.46 learn_time(ms) 25881.59 saved /tmp/ppo/taxi_3/checkpoint_7/checkpoint-7\n",
      "  8 reward -1010.00/-677.99/-170.00 len 188.51 learn_time(ms) 25789.05 saved /tmp/ppo/taxi_3/checkpoint_8/checkpoint-8\n",
      "  9 reward -1010.00/-646.23/-166.00 len 183.93 learn_time(ms) 25743.30 saved /tmp/ppo/taxi_3/checkpoint_9/checkpoint-9\n",
      " 10 reward -1010.00/-606.20/-127.00 len 179.06 learn_time(ms) 26102.21 saved /tmp/ppo/taxi_3/checkpoint_10/checkpoint-10\n",
      " 11 reward -929.00/-562.49/-60.00 len 170.60 learn_time(ms) 26588.17 saved /tmp/ppo/taxi_3/checkpoint_11/checkpoint-11\n",
      " 12 reward -929.00/-545.64/-60.00 len 170.43 learn_time(ms) 26478.85 saved /tmp/ppo/taxi_3/checkpoint_12/checkpoint-12\n",
      " 13 reward -929.00/-521.71/-58.00 len 167.11 learn_time(ms) 26413.11 saved /tmp/ppo/taxi_3/checkpoint_13/checkpoint-13\n",
      " 14 reward -929.00/-462.29/-54.00 len 156.26 learn_time(ms) 26435.64 saved /tmp/ppo/taxi_3/checkpoint_14/checkpoint-14\n",
      " 15 reward -929.00/-440.33/-33.00 len 155.27 learn_time(ms) 26318.82 saved /tmp/ppo/taxi_3/checkpoint_15/checkpoint-15\n",
      " 16 reward -929.00/-361.24/-33.00 len 136.06 learn_time(ms) 26261.78 saved /tmp/ppo/taxi_3/checkpoint_16/checkpoint-16\n",
      " 17 reward -704.00/-311.89/-10.00 len 121.57 learn_time(ms) 26366.00 saved /tmp/ppo/taxi_3/checkpoint_17/checkpoint-17\n",
      " 18 reward -686.00/-285.71/-10.00 len 117.41 learn_time(ms) 26465.51 saved /tmp/ppo/taxi_3/checkpoint_18/checkpoint-18\n",
      " 19 reward -812.00/-292.23/-10.00 len 122.25 learn_time(ms) 26706.43 saved /tmp/ppo/taxi_3/checkpoint_19/checkpoint-19\n",
      " 20 reward -812.00/-285.97/-13.00 len 126.07 learn_time(ms) 26331.48 saved /tmp/ppo/taxi_3/checkpoint_20/checkpoint-20\n",
      " 21 reward -812.00/-263.61/ 12.00 len 121.86 learn_time(ms) 25906.58 saved /tmp/ppo/taxi_3/checkpoint_21/checkpoint-21\n",
      " 22 reward -578.00/-192.88/ 12.00 len 101.71 learn_time(ms) 26070.60 saved /tmp/ppo/taxi_3/checkpoint_22/checkpoint-22\n",
      " 23 reward -560.00/-167.58/  0.00 len  96.51 learn_time(ms) 26170.63 saved /tmp/ppo/taxi_3/checkpoint_23/checkpoint-23\n",
      " 24 reward -551.00/-150.18/  7.00 len  90.60 learn_time(ms) 26127.38 saved /tmp/ppo/taxi_3/checkpoint_24/checkpoint-24\n",
      " 25 reward -551.00/-142.85/  7.00 len  87.38 learn_time(ms) 26220.40 saved /tmp/ppo/taxi_3/checkpoint_25/checkpoint-25\n",
      " 26 reward -524.00/-135.94/  2.00 len  84.37 learn_time(ms) 26330.39 saved /tmp/ppo/taxi_3/checkpoint_26/checkpoint-26\n",
      " 27 reward -416.00/-109.72/  9.00 len  76.66 learn_time(ms) 26266.52 saved /tmp/ppo/taxi_3/checkpoint_27/checkpoint-27\n",
      " 28 reward -353.00/-86.54/  9.00 len  68.30 learn_time(ms) 26196.52 saved /tmp/ppo/taxi_3/checkpoint_28/checkpoint-28\n",
      " 29 reward -407.00/-85.04/  9.00 len  66.53 learn_time(ms) 26343.11 saved /tmp/ppo/taxi_3/checkpoint_29/checkpoint-29\n",
      " 30 reward -407.00/-85.46/  9.00 len  69.77 learn_time(ms) 26343.03 saved /tmp/ppo/taxi_3/checkpoint_30/checkpoint-30\n",
      "Total learn time: 785302.7760000001\n",
      "Average learn time per iteration: 26176.759200000004\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 1024)         513024      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 1024)         513024      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 1024)         1049600     fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 1024)         1049600     fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 6)            6150        fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            1025        fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,132,423\n",
      "Trainable params: 3,132,423\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Variables\n",
      "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(500, 1024) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_1/bias:0' shape=(1024,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(500, 1024) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(1024,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(1024, 1024) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/bias:0' shape=(1024,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(1024, 1024) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(1024,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(1024, 6) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/bias:0' shape=(6,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/kernel:0' shape=(1024, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
      "Value function\n",
      "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _metrics_export_port=63419)\n",
    "checkpoint_root = '/tmp/ppo/taxi_3'\n",
    "env = 'Taxi-v3'\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['model']['fcnet_hiddens'] = [1024,1024]\n",
    "n_iter = 30\n",
    "full_train(checkpoint_root, env, config, n_iter)\n",
    "ray.timeline(\"/mnt/c/Users/javig/timelines/time_taxi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo con tres capas internas de 256 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 19:41:08,392\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=4074)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4074)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4074)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=4076)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4076)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4076)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-19 19:41:19,373\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=4076)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4076)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4076)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=4074)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4074)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4074)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward -929.00/-794.00/-677.00 len 200.00 learn_time(ms) 5309.74 saved /tmp/ppo/taxi_2/checkpoint_1/checkpoint-1\n",
      "  2 reward -929.00/-765.15/-369.00 len 195.90 learn_time(ms) 5407.62 saved /tmp/ppo/taxi_2/checkpoint_2/checkpoint-2\n",
      "  3 reward -929.00/-767.27/-369.00 len 197.27 learn_time(ms) 5661.07 saved /tmp/ppo/taxi_2/checkpoint_3/checkpoint-3\n",
      "  4 reward -929.00/-762.02/-369.00 len 197.95 learn_time(ms) 5596.58 saved /tmp/ppo/taxi_2/checkpoint_4/checkpoint-4\n",
      "  5 reward -929.00/-756.42/-369.00 len 198.18 learn_time(ms) 5504.32 saved /tmp/ppo/taxi_2/checkpoint_5/checkpoint-5\n",
      "  6 reward -965.00/-722.41/-118.00 len 194.38 learn_time(ms) 5443.56 saved /tmp/ppo/taxi_2/checkpoint_6/checkpoint-6\n",
      "  7 reward -965.00/-692.99/-81.00 len 191.09 learn_time(ms) 5390.56 saved /tmp/ppo/taxi_2/checkpoint_7/checkpoint-7\n",
      "  8 reward -965.00/-644.77/-37.00 len 182.83 learn_time(ms) 5350.81 saved /tmp/ppo/taxi_2/checkpoint_8/checkpoint-8\n",
      "  9 reward -965.00/-593.79/-37.00 len 174.27 learn_time(ms) 5403.76 saved /tmp/ppo/taxi_2/checkpoint_9/checkpoint-9\n",
      " 10 reward -884.00/-573.58/-37.00 len 174.73 learn_time(ms) 5381.36 saved /tmp/ppo/taxi_2/checkpoint_10/checkpoint-10\n",
      " 11 reward -884.00/-542.36/-14.00 len 170.48 learn_time(ms) 5401.26 saved /tmp/ppo/taxi_2/checkpoint_11/checkpoint-11\n",
      " 12 reward -848.00/-541.71/-14.00 len 174.15 learn_time(ms) 5596.35 saved /tmp/ppo/taxi_2/checkpoint_12/checkpoint-12\n",
      " 13 reward -812.00/-535.24/-14.00 len 178.33 learn_time(ms) 5561.80 saved /tmp/ppo/taxi_2/checkpoint_13/checkpoint-13\n",
      " 14 reward -1073.00/-543.60/-14.00 len 179.91 learn_time(ms) 5633.21 saved /tmp/ppo/taxi_2/checkpoint_14/checkpoint-14\n",
      " 15 reward -1073.00/-539.20/-14.00 len 179.86 learn_time(ms) 5717.45 saved /tmp/ppo/taxi_2/checkpoint_15/checkpoint-15\n",
      " 16 reward -1073.00/-528.37/-22.00 len 179.62 learn_time(ms) 5783.58 saved /tmp/ppo/taxi_2/checkpoint_16/checkpoint-16\n",
      " 17 reward -1073.00/-479.34/-14.00 len 169.95 learn_time(ms) 5865.16 saved /tmp/ppo/taxi_2/checkpoint_17/checkpoint-17\n",
      " 18 reward -965.00/-422.00/ -7.00 len 160.79 learn_time(ms) 5865.30 saved /tmp/ppo/taxi_2/checkpoint_18/checkpoint-18\n",
      " 19 reward -965.00/-359.26/ -7.00 len 148.00 learn_time(ms) 5847.98 saved /tmp/ppo/taxi_2/checkpoint_19/checkpoint-19\n",
      " 20 reward -920.00/-333.99/ -2.00 len 141.03 learn_time(ms) 5909.37 saved /tmp/ppo/taxi_2/checkpoint_20/checkpoint-20\n",
      " 21 reward -920.00/-309.45/  6.00 len 134.22 learn_time(ms) 5900.78 saved /tmp/ppo/taxi_2/checkpoint_21/checkpoint-21\n",
      " 22 reward -920.00/-281.80/  6.00 len 126.04 learn_time(ms) 5683.80 saved /tmp/ppo/taxi_2/checkpoint_22/checkpoint-22\n",
      " 23 reward -695.00/-261.04/ 11.00 len 126.67 learn_time(ms) 5615.43 saved /tmp/ppo/taxi_2/checkpoint_23/checkpoint-23\n",
      " 24 reward -695.00/-250.04/ 11.00 len 127.01 learn_time(ms) 5526.87 saved /tmp/ppo/taxi_2/checkpoint_24/checkpoint-24\n",
      " 25 reward -641.00/-228.13/ 12.00 len 120.58 learn_time(ms) 5465.00 saved /tmp/ppo/taxi_2/checkpoint_25/checkpoint-25\n",
      " 26 reward -605.00/-223.64/ 12.00 len 119.33 learn_time(ms) 5563.59 saved /tmp/ppo/taxi_2/checkpoint_26/checkpoint-26\n",
      " 27 reward -605.00/-224.28/ 12.00 len 120.48 learn_time(ms) 5640.54 saved /tmp/ppo/taxi_2/checkpoint_27/checkpoint-27\n",
      " 28 reward -569.00/-219.61/  7.00 len 124.33 learn_time(ms) 5673.71 saved /tmp/ppo/taxi_2/checkpoint_28/checkpoint-28\n",
      " 29 reward -515.00/-214.53/  7.00 len 126.03 learn_time(ms) 5642.90 saved /tmp/ppo/taxi_2/checkpoint_29/checkpoint-29\n",
      " 30 reward -506.00/-209.56/  5.00 len 127.75 learn_time(ms) 5599.57 saved /tmp/ppo/taxi_2/checkpoint_30/checkpoint-30\n",
      "Total learn time: 167943.05800000005\n",
      "Average learn time per iteration: 5598.101933333335\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          128256      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          128256      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_3 (Dense)                    (None, 256)          65792       fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_3 (Dense)              (None, 256)          65792       fc_value_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 6)            1542        fc_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_3[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 521,479\n",
      "Trainable params: 521,479\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Variables\n",
      "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(500, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(500, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_3/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_3/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_3/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_3/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(256, 6) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/bias:0' shape=(6,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/kernel:0' shape=(256, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
      "Value function\n",
      "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _metrics_export_port=63419)\n",
    "checkpoint_root = '/tmp/ppo/taxi_2'\n",
    "env = 'Taxi-v3'\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['model']['fcnet_hiddens'] = [256,256,256]\n",
    "n_iter = 30\n",
    "full_train(checkpoint_root, env, config, n_iter)\n",
    "ray.timeline(\"/mnt/c/Users/javig/timelines/time_taxi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo con tres capas internas de 512 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 19:45:29,985\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=4272)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4272)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4272)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=4274)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4274)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4274)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-19 19:45:40,494\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=4272)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4272)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4272)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=4274)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4274)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4274)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward -956.00/-792.20/-695.00 len 200.00 learn_time(ms) 13189.22 saved /tmp/ppo/taxi_4/checkpoint_1/checkpoint-1\n",
      "  2 reward -956.00/-773.80/-175.00 len 196.75 learn_time(ms) 13697.33 saved /tmp/ppo/taxi_4/checkpoint_2/checkpoint-2\n",
      "  3 reward -956.00/-758.18/-175.00 len 197.83 learn_time(ms) 13476.13 saved /tmp/ppo/taxi_4/checkpoint_3/checkpoint-3\n",
      "  4 reward -956.00/-740.06/-175.00 len 196.62 learn_time(ms) 13430.25 saved /tmp/ppo/taxi_4/checkpoint_4/checkpoint-4\n",
      "  5 reward -956.00/-738.22/-175.00 len 196.72 learn_time(ms) 13395.33 saved /tmp/ppo/taxi_4/checkpoint_5/checkpoint-5\n",
      "  6 reward -956.00/-714.26/-217.00 len 196.67 learn_time(ms) 13362.52 saved /tmp/ppo/taxi_4/checkpoint_6/checkpoint-6\n",
      "  7 reward -956.00/-678.76/-100.00 len 194.11 learn_time(ms) 13376.89 saved /tmp/ppo/taxi_4/checkpoint_7/checkpoint-7\n",
      "  8 reward -983.00/-654.51/-100.00 len 190.86 learn_time(ms) 13333.97 saved /tmp/ppo/taxi_4/checkpoint_8/checkpoint-8\n",
      "  9 reward -983.00/-647.96/-100.00 len 189.62 learn_time(ms) 13346.19 saved /tmp/ppo/taxi_4/checkpoint_9/checkpoint-9\n",
      " 10 reward -983.00/-621.51/-100.00 len 189.21 learn_time(ms) 13322.33 saved /tmp/ppo/taxi_4/checkpoint_10/checkpoint-10\n",
      " 11 reward -983.00/-605.99/-100.00 len 189.53 learn_time(ms) 13319.15 saved /tmp/ppo/taxi_4/checkpoint_11/checkpoint-11\n",
      " 12 reward -983.00/-586.68/-103.00 len 185.97 learn_time(ms) 13215.59 saved /tmp/ppo/taxi_4/checkpoint_12/checkpoint-12\n",
      " 13 reward -857.00/-564.93/-80.00 len 185.01 learn_time(ms) 13215.69 saved /tmp/ppo/taxi_4/checkpoint_13/checkpoint-13\n",
      " 14 reward -857.00/-508.25/-16.00 len 176.36 learn_time(ms) 13187.27 saved /tmp/ppo/taxi_4/checkpoint_14/checkpoint-14\n",
      " 15 reward -857.00/-464.17/-16.00 len 167.86 learn_time(ms) 13167.77 saved /tmp/ppo/taxi_4/checkpoint_15/checkpoint-15\n",
      " 16 reward -857.00/-433.41/-16.00 len 165.93 learn_time(ms) 13175.14 saved /tmp/ppo/taxi_4/checkpoint_16/checkpoint-16\n",
      " 17 reward -758.00/-389.07/-16.00 len 160.20 learn_time(ms) 13167.42 saved /tmp/ppo/taxi_4/checkpoint_17/checkpoint-17\n",
      " 18 reward -758.00/-373.07/ -3.00 len 158.42 learn_time(ms) 13249.22 saved /tmp/ppo/taxi_4/checkpoint_18/checkpoint-18\n",
      " 19 reward -758.00/-347.03/  0.00 len 150.74 learn_time(ms) 13229.96 saved /tmp/ppo/taxi_4/checkpoint_19/checkpoint-19\n",
      " 20 reward -767.00/-328.94/  0.00 len 144.74 learn_time(ms) 13476.59 saved /tmp/ppo/taxi_4/checkpoint_20/checkpoint-20\n",
      " 21 reward -767.00/-326.45/  0.00 len 144.80 learn_time(ms) 13607.86 saved /tmp/ppo/taxi_4/checkpoint_21/checkpoint-21\n",
      " 22 reward -767.00/-311.35/-12.00 len 145.18 learn_time(ms) 13635.90 saved /tmp/ppo/taxi_4/checkpoint_22/checkpoint-22\n",
      " 23 reward -767.00/-295.97/  3.00 len 143.33 learn_time(ms) 13692.41 saved /tmp/ppo/taxi_4/checkpoint_23/checkpoint-23\n",
      " 24 reward -794.00/-271.81/  5.00 len 140.35 learn_time(ms) 13779.80 saved /tmp/ppo/taxi_4/checkpoint_24/checkpoint-24\n",
      " 25 reward -794.00/-227.74/ 13.00 len 122.20 learn_time(ms) 13870.80 saved /tmp/ppo/taxi_4/checkpoint_25/checkpoint-25\n",
      " 26 reward -659.00/-192.70/ 13.00 len 111.01 learn_time(ms) 13884.15 saved /tmp/ppo/taxi_4/checkpoint_26/checkpoint-26\n",
      " 27 reward -659.00/-181.82/ 13.00 len 109.31 learn_time(ms) 13887.69 saved /tmp/ppo/taxi_4/checkpoint_27/checkpoint-27\n",
      " 28 reward -659.00/-171.63/  5.00 len 107.22 learn_time(ms) 13923.33 saved /tmp/ppo/taxi_4/checkpoint_28/checkpoint-28\n",
      " 29 reward -488.00/-158.93/  5.00 len 106.76 learn_time(ms) 13991.37 saved /tmp/ppo/taxi_4/checkpoint_29/checkpoint-29\n",
      " 30 reward -416.00/-138.94/  5.00 len  99.64 learn_time(ms) 13919.06 saved /tmp/ppo/taxi_4/checkpoint_30/checkpoint-30\n",
      "Total learn time: 404526.34699999995\n",
      "Average learn time per iteration: 13484.211566666665\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 512)          256512      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 512)          256512      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 512)          262656      fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 512)          262656      fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_3 (Dense)                    (None, 512)          262656      fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_3 (Dense)              (None, 512)          262656      fc_value_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 6)            3078        fc_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            513         fc_value_3[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,567,239\n",
      "Trainable params: 1,567,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Variables\n",
      "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(500, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_1/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(500, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(512, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(512, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_3/kernel:0' shape=(512, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_3/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_3/kernel:0' shape=(512, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_3/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(512, 6) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/bias:0' shape=(6,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/kernel:0' shape=(512, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
      "Value function\n",
      "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _metrics_export_port=63419)\n",
    "checkpoint_root = '/tmp/ppo/taxi_4'\n",
    "env = 'Taxi-v3'\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['model']['fcnet_hiddens'] = [512,512,512]\n",
    "n_iter = 30\n",
    "full_train(checkpoint_root, env, config, n_iter)\n",
    "ray.timeline(\"/mnt/c/Users/javig/timelines/time_taxi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo con tres capas internas de 1024 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 21:01:12,454\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2020-11-19 21:01:14,973\tINFO trainer.py:592 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-11-19 21:01:14,974\tINFO trainer.py:1064 -- `_use_trajectory_view_api` only supported for PyTorch so far! Will run w/o.\n",
      "2020-11-19 21:01:14,975\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=7592)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=7592)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=7592)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=7596)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=7596)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=7596)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-19 21:01:23,589\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=7596)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=7596)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=7596)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=7592)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=7592)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=7592)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward -884.00/-810.65/-677.00 len 200.00 learn_time(ms) 41042.34 saved /tmp/ppo/taxi_5/checkpoint_1/checkpoint-1\n",
      "  2 reward -884.00/-785.00/-677.00 len 200.00 learn_time(ms) 41152.00 saved /tmp/ppo/taxi_5/checkpoint_2/checkpoint-2\n",
      "  3 reward -893.00/-753.02/-250.00 len 194.87 learn_time(ms) 41330.16 saved /tmp/ppo/taxi_5/checkpoint_3/checkpoint-3\n",
      "  4 reward -893.00/-747.06/-250.00 len 196.15 learn_time(ms) 41324.88 saved /tmp/ppo/taxi_5/checkpoint_4/checkpoint-4\n",
      "  5 reward -1019.00/-749.56/-250.00 len 196.90 learn_time(ms) 41342.14 saved /tmp/ppo/taxi_5/checkpoint_5/checkpoint-5\n",
      "  6 reward -1019.00/-726.07/-250.00 len 196.90 learn_time(ms) 41491.87 saved /tmp/ppo/taxi_5/checkpoint_6/checkpoint-6\n",
      "  7 reward -1019.00/-699.00/-91.00 len 193.83 learn_time(ms) 41522.85 saved /tmp/ppo/taxi_5/checkpoint_7/checkpoint-7\n",
      "  8 reward -1019.00/-663.07/-63.00 len 191.38 learn_time(ms) 42810.95 saved /tmp/ppo/taxi_5/checkpoint_8/checkpoint-8\n",
      "  9 reward -956.00/-636.42/-63.00 len 189.03 learn_time(ms) 43882.47 saved /tmp/ppo/taxi_5/checkpoint_9/checkpoint-9\n",
      " 10 reward -929.00/-603.31/-63.00 len 185.17 learn_time(ms) 43998.45 saved /tmp/ppo/taxi_5/checkpoint_10/checkpoint-10\n",
      " 11 reward -929.00/-578.42/-63.00 len 183.74 learn_time(ms) 44069.17 saved /tmp/ppo/taxi_5/checkpoint_11/checkpoint-11\n",
      " 12 reward -866.00/-573.96/-88.00 len 184.92 learn_time(ms) 44108.52 saved /tmp/ppo/taxi_5/checkpoint_12/checkpoint-12\n",
      " 13 reward -866.00/-515.41/-41.00 len 171.19 learn_time(ms) 44129.68 saved /tmp/ppo/taxi_5/checkpoint_13/checkpoint-13\n",
      " 14 reward -866.00/-486.75/-41.00 len 168.03 learn_time(ms) 44161.76 saved /tmp/ppo/taxi_5/checkpoint_14/checkpoint-14\n",
      " 15 reward -866.00/-459.84/ -4.00 len 164.76 learn_time(ms) 44446.08 saved /tmp/ppo/taxi_5/checkpoint_15/checkpoint-15\n",
      " 16 reward -866.00/-427.94/  3.00 len 160.94 learn_time(ms) 45351.56 saved /tmp/ppo/taxi_5/checkpoint_16/checkpoint-16\n",
      " 17 reward -821.00/-414.66/ 14.00 len 159.18 learn_time(ms) 46300.90 saved /tmp/ppo/taxi_5/checkpoint_17/checkpoint-17\n",
      " 18 reward -821.00/-387.02/ 14.00 len 155.99 learn_time(ms) 45333.73 saved /tmp/ppo/taxi_5/checkpoint_18/checkpoint-18\n",
      " 19 reward -866.00/-361.64/ 14.00 len 151.70 learn_time(ms) 44277.82 saved /tmp/ppo/taxi_5/checkpoint_19/checkpoint-19\n",
      " 20 reward -866.00/-339.31/ -3.00 len 148.78 learn_time(ms) 43951.26 saved /tmp/ppo/taxi_5/checkpoint_20/checkpoint-20\n",
      " 21 reward -866.00/-283.15/ 10.00 len 133.42 learn_time(ms) 43935.01 saved /tmp/ppo/taxi_5/checkpoint_21/checkpoint-21\n",
      " 22 reward -659.00/-236.89/ 10.00 len 117.67 learn_time(ms) 43937.42 saved /tmp/ppo/taxi_5/checkpoint_22/checkpoint-22\n",
      " 23 reward -659.00/-206.57/ 10.00 len 109.46 learn_time(ms) 43904.45 saved /tmp/ppo/taxi_5/checkpoint_23/checkpoint-23\n",
      " 24 reward -614.00/-216.75/  9.00 len 119.07 learn_time(ms) 43902.98 saved /tmp/ppo/taxi_5/checkpoint_24/checkpoint-24\n",
      " 25 reward -596.00/-228.62/ 10.00 len 128.93 learn_time(ms) 43836.41 saved /tmp/ppo/taxi_5/checkpoint_25/checkpoint-25\n",
      " 26 reward -596.00/-216.09/ 10.00 len 122.04 learn_time(ms) 42938.56 saved /tmp/ppo/taxi_5/checkpoint_26/checkpoint-26\n",
      " 27 reward -578.00/-175.31/ 10.00 len 100.19 learn_time(ms) 41986.54 saved /tmp/ppo/taxi_5/checkpoint_27/checkpoint-27\n",
      " 28 reward -578.00/-139.24/ 10.00 len  88.36 learn_time(ms) 42296.67 saved /tmp/ppo/taxi_5/checkpoint_28/checkpoint-28\n",
      " 29 reward -515.00/-153.17/ 10.00 len  94.64 learn_time(ms) 43171.42 saved /tmp/ppo/taxi_5/checkpoint_29/checkpoint-29\n",
      " 30 reward -515.00/-149.20/  9.00 len  92.29 learn_time(ms) 44123.07 saved /tmp/ppo/taxi_5/checkpoint_30/checkpoint-30\n",
      "Total learn time: 1300061.112\n",
      "Average learn time per iteration: 43335.3704\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 1024)         513024      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 1024)         513024      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 1024)         1049600     fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 1024)         1049600     fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_3 (Dense)                    (None, 1024)         1049600     fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_3 (Dense)              (None, 1024)         1049600     fc_value_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 6)            6150        fc_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            1025        fc_value_3[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 5,231,623\n",
      "Trainable params: 5,231,623\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Variables\n",
      "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(500, 1024) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_1/bias:0' shape=(1024,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(500, 1024) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(1024,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(1024, 1024) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/bias:0' shape=(1024,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(1024, 1024) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(1024,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_3/kernel:0' shape=(1024, 1024) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_3/bias:0' shape=(1024,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_3/kernel:0' shape=(1024, 1024) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_3/bias:0' shape=(1024,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(1024, 6) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/bias:0' shape=(6,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/kernel:0' shape=(1024, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
      "Value function\n",
      "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _metrics_export_port=63419)\n",
    "checkpoint_root = '/tmp/ppo/taxi_5'\n",
    "env = 'Taxi-v3'\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['model']['fcnet_hiddens'] = [1024,1024,1024]\n",
    "n_iter = 30\n",
    "full_train(checkpoint_root, env, config, n_iter)\n",
    "ray.timeline(\"/mnt/c/Users/javig/timelines/time_taxi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo con tres capas internas con la estructura \\[256, 512, 256\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 21:26:51,364\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=8969)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8969)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8969)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8971)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8971)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8971)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-19 21:27:01,476\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=8971)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8971)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8971)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=8969)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8969)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8969)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward -857.00/-742.65/-82.00 len 192.00 learn_time(ms) 7505.29 saved /tmp/ppo/taxi_6/checkpoint_1/checkpoint-1\n",
      "  2 reward -875.00/-730.33/-82.00 len 193.25 learn_time(ms) 7317.44 saved /tmp/ppo/taxi_6/checkpoint_2/checkpoint-2\n",
      "  3 reward -875.00/-719.39/-82.00 len 192.89 learn_time(ms) 7240.03 saved /tmp/ppo/taxi_6/checkpoint_3/checkpoint-3\n",
      "  4 reward -902.00/-726.38/-82.00 len 194.24 learn_time(ms) 7206.93 saved /tmp/ppo/taxi_6/checkpoint_4/checkpoint-4\n",
      "  5 reward -902.00/-724.15/-262.00 len 196.45 learn_time(ms) 7180.29 saved /tmp/ppo/taxi_6/checkpoint_5/checkpoint-5\n",
      "  6 reward -902.00/-693.88/-170.00 len 195.10 learn_time(ms) 7163.70 saved /tmp/ppo/taxi_6/checkpoint_6/checkpoint-6\n",
      "  7 reward -902.00/-678.65/-137.00 len 194.36 learn_time(ms) 7158.40 saved /tmp/ppo/taxi_6/checkpoint_7/checkpoint-7\n",
      "  8 reward -902.00/-654.40/-137.00 len 192.43 learn_time(ms) 7151.22 saved /tmp/ppo/taxi_6/checkpoint_8/checkpoint-8\n",
      "  9 reward -902.00/-609.08/-101.00 len 187.19 learn_time(ms) 7145.72 saved /tmp/ppo/taxi_6/checkpoint_9/checkpoint-9\n",
      " 10 reward -902.00/-573.73/-101.00 len 184.66 learn_time(ms) 7153.24 saved /tmp/ppo/taxi_6/checkpoint_10/checkpoint-10\n",
      " 11 reward -902.00/-539.10/-49.00 len 181.62 learn_time(ms) 7111.26 saved /tmp/ppo/taxi_6/checkpoint_11/checkpoint-11\n",
      " 12 reward -902.00/-509.54/-49.00 len 180.35 learn_time(ms) 7108.66 saved /tmp/ppo/taxi_6/checkpoint_12/checkpoint-12\n",
      " 13 reward -893.00/-476.80/-22.00 len 178.96 learn_time(ms) 7113.49 saved /tmp/ppo/taxi_6/checkpoint_13/checkpoint-13\n",
      " 14 reward -893.00/-450.29/-22.00 len 173.72 learn_time(ms) 7118.79 saved /tmp/ppo/taxi_6/checkpoint_14/checkpoint-14\n",
      " 15 reward -893.00/-431.37/-22.00 len 169.59 learn_time(ms) 7124.37 saved /tmp/ppo/taxi_6/checkpoint_15/checkpoint-15\n",
      " 16 reward -803.00/-411.15/-15.00 len 164.22 learn_time(ms) 7128.40 saved /tmp/ppo/taxi_6/checkpoint_16/checkpoint-16\n",
      " 17 reward -803.00/-392.41/-15.00 len 159.04 learn_time(ms) 7123.43 saved /tmp/ppo/taxi_6/checkpoint_17/checkpoint-17\n",
      " 18 reward -749.00/-371.96/-14.00 len 154.19 learn_time(ms) 7123.65 saved /tmp/ppo/taxi_6/checkpoint_18/checkpoint-18\n",
      " 19 reward -749.00/-337.02/  9.00 len 148.71 learn_time(ms) 7123.45 saved /tmp/ppo/taxi_6/checkpoint_19/checkpoint-19\n",
      " 20 reward -749.00/-321.55/  9.00 len 147.97 learn_time(ms) 7112.88 saved /tmp/ppo/taxi_6/checkpoint_20/checkpoint-20\n",
      " 21 reward -659.00/-298.09/ 10.00 len 148.36 learn_time(ms) 7114.41 saved /tmp/ppo/taxi_6/checkpoint_21/checkpoint-21\n",
      " 22 reward -632.00/-279.78/ 10.00 len 147.18 learn_time(ms) 7111.25 saved /tmp/ppo/taxi_6/checkpoint_22/checkpoint-22\n",
      " 23 reward -569.00/-270.54/ 10.00 len 147.96 learn_time(ms) 7105.16 saved /tmp/ppo/taxi_6/checkpoint_23/checkpoint-23\n",
      " 24 reward -650.00/-246.20/ 10.00 len 141.83 learn_time(ms) 7098.74 saved /tmp/ppo/taxi_6/checkpoint_24/checkpoint-24\n",
      " 25 reward -650.00/-221.78/ 10.00 len 136.01 learn_time(ms) 7103.73 saved /tmp/ppo/taxi_6/checkpoint_25/checkpoint-25\n",
      " 26 reward -650.00/-211.40/ 10.00 len 134.45 learn_time(ms) 7109.95 saved /tmp/ppo/taxi_6/checkpoint_26/checkpoint-26\n",
      " 27 reward -650.00/-206.11/ 10.00 len 134.02 learn_time(ms) 7110.71 saved /tmp/ppo/taxi_6/checkpoint_27/checkpoint-27\n",
      " 28 reward -569.00/-212.44/  6.00 len 138.76 learn_time(ms) 7114.94 saved /tmp/ppo/taxi_6/checkpoint_28/checkpoint-28\n",
      " 29 reward -533.00/-206.13/  5.00 len 141.24 learn_time(ms) 7118.76 saved /tmp/ppo/taxi_6/checkpoint_29/checkpoint-29\n",
      " 30 reward -434.00/-188.43/  5.00 len 130.14 learn_time(ms) 7115.55 saved /tmp/ppo/taxi_6/checkpoint_30/checkpoint-30\n",
      "Total learn time: 214513.83599999998\n",
      "Average learn time per iteration: 7150.4612\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          128256      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          128256      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 512)          131584      fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 512)          131584      fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_3 (Dense)                    (None, 256)          131328      fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_3 (Dense)              (None, 256)          131328      fc_value_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 6)            1542        fc_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_3[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 784,135\n",
      "Trainable params: 784,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Variables\n",
      "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(500, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(500, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(256, 512) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(512,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_3/kernel:0' shape=(512, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_3/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_3/kernel:0' shape=(512, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_3/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(256, 6) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/bias:0' shape=(6,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/kernel:0' shape=(256, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
      "Value function\n",
      "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _metrics_export_port=63419)\n",
    "checkpoint_root = '/tmp/ppo/taxi_6'\n",
    "env = 'Taxi-v3'\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['model']['fcnet_hiddens'] = [256,512,256]\n",
    "n_iter = 30\n",
    "full_train(checkpoint_root, env, config, n_iter)\n",
    "ray.timeline(\"/mnt/c/Users/javig/timelines/time_taxi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo con cuatro capas internas de 256 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 23:15:30,826\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=9544)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9544)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9544)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9547)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9547)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9547)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-19 23:15:41,826\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=9547)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9547)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9547)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=9544)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9544)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9544)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward -893.00/-789.05/-713.00 len 200.00 learn_time(ms) 6924.62 saved /tmp/ppo/taxi_7/checkpoint_1/checkpoint-1\n",
      "  2 reward -893.00/-779.83/-605.00 len 200.00 learn_time(ms) 6892.72 saved /tmp/ppo/taxi_7/checkpoint_2/checkpoint-2\n",
      "  3 reward -920.00/-774.42/-605.00 len 199.22 learn_time(ms) 6738.23 saved /tmp/ppo/taxi_7/checkpoint_3/checkpoint-3\n",
      "  4 reward -947.00/-770.16/-332.00 len 197.84 learn_time(ms) 6676.10 saved /tmp/ppo/taxi_7/checkpoint_4/checkpoint-4\n",
      "  5 reward -974.00/-759.60/-111.00 len 196.05 learn_time(ms) 6728.55 saved /tmp/ppo/taxi_7/checkpoint_5/checkpoint-5\n",
      "  6 reward -1010.00/-730.05/-111.00 len 191.73 learn_time(ms) 6731.40 saved /tmp/ppo/taxi_7/checkpoint_6/checkpoint-6\n",
      "  7 reward -1010.00/-710.84/-111.00 len 192.20 learn_time(ms) 6756.06 saved /tmp/ppo/taxi_7/checkpoint_7/checkpoint-7\n",
      "  8 reward -1010.00/-678.06/-111.00 len 188.94 learn_time(ms) 6691.53 saved /tmp/ppo/taxi_7/checkpoint_8/checkpoint-8\n",
      "  9 reward -1010.00/-644.73/-111.00 len 187.83 learn_time(ms) 6644.51 saved /tmp/ppo/taxi_7/checkpoint_9/checkpoint-9\n",
      " 10 reward -1010.00/-621.87/-117.00 len 185.64 learn_time(ms) 6617.30 saved /tmp/ppo/taxi_7/checkpoint_10/checkpoint-10\n",
      " 11 reward -1010.00/-598.29/-118.00 len 186.75 learn_time(ms) 6579.97 saved /tmp/ppo/taxi_7/checkpoint_11/checkpoint-11\n",
      " 12 reward -1010.00/-553.05/-111.00 len 180.24 learn_time(ms) 6721.50 saved /tmp/ppo/taxi_7/checkpoint_12/checkpoint-12\n",
      " 13 reward -875.00/-534.98/-111.00 len 182.75 learn_time(ms) 6732.02 saved /tmp/ppo/taxi_7/checkpoint_13/checkpoint-13\n",
      " 14 reward -875.00/-522.29/-111.00 len 186.29 learn_time(ms) 6781.43 saved /tmp/ppo/taxi_7/checkpoint_14/checkpoint-14\n",
      " 15 reward -821.00/-488.05/-30.00 len 185.17 learn_time(ms) 6734.14 saved /tmp/ppo/taxi_7/checkpoint_15/checkpoint-15\n",
      " 16 reward -821.00/-464.74/-30.00 len 182.08 learn_time(ms) 6702.17 saved /tmp/ppo/taxi_7/checkpoint_16/checkpoint-16\n",
      " 17 reward -848.00/-433.20/ -7.00 len 173.91 learn_time(ms) 6660.43 saved /tmp/ppo/taxi_7/checkpoint_17/checkpoint-17\n",
      " 18 reward -848.00/-423.56/ -7.00 len 169.76 learn_time(ms) 6722.29 saved /tmp/ppo/taxi_7/checkpoint_18/checkpoint-18\n",
      " 19 reward -848.00/-384.26/  9.00 len 159.68 learn_time(ms) 6781.95 saved /tmp/ppo/taxi_7/checkpoint_19/checkpoint-19\n",
      " 20 reward -848.00/-366.58/  9.00 len 157.00 learn_time(ms) 6834.19 saved /tmp/ppo/taxi_7/checkpoint_20/checkpoint-20\n",
      " 21 reward -713.00/-375.01/  9.00 len 164.38 learn_time(ms) 6805.72 saved /tmp/ppo/taxi_7/checkpoint_21/checkpoint-21\n",
      " 22 reward -713.00/-353.26/  9.00 len 163.87 learn_time(ms) 6605.02 saved /tmp/ppo/taxi_7/checkpoint_22/checkpoint-22\n",
      " 23 reward -794.00/-352.66/  9.00 len 166.39 learn_time(ms) 6644.42 saved /tmp/ppo/taxi_7/checkpoint_23/checkpoint-23\n",
      " 24 reward -794.00/-350.95/  9.00 len 171.13 learn_time(ms) 6601.05 saved /tmp/ppo/taxi_7/checkpoint_24/checkpoint-24\n",
      " 25 reward -794.00/-321.51/ 14.00 len 170.79 learn_time(ms) 6584.73 saved /tmp/ppo/taxi_7/checkpoint_25/checkpoint-25\n",
      " 26 reward -794.00/-286.39/ 14.00 len 163.93 learn_time(ms) 6570.64 saved /tmp/ppo/taxi_7/checkpoint_26/checkpoint-26\n",
      " 27 reward -623.00/-272.08/ 14.00 len 161.95 learn_time(ms) 6572.14 saved /tmp/ppo/taxi_7/checkpoint_27/checkpoint-27\n",
      " 28 reward -623.00/-250.39/ 14.00 len 156.13 learn_time(ms) 6598.58 saved /tmp/ppo/taxi_7/checkpoint_28/checkpoint-28\n",
      " 29 reward -623.00/-238.85/  6.00 len 153.56 learn_time(ms) 6540.39 saved /tmp/ppo/taxi_7/checkpoint_29/checkpoint-29\n",
      " 30 reward -623.00/-241.99/  7.00 len 159.13 learn_time(ms) 6478.75 saved /tmp/ppo/taxi_7/checkpoint_30/checkpoint-30\n",
      "Total learn time: 200652.526\n",
      "Average learn time per iteration: 6688.417533333334\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          128256      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          128256      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_3 (Dense)                    (None, 256)          65792       fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_3 (Dense)              (None, 256)          65792       fc_value_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_4 (Dense)                    (None, 256)          65792       fc_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_4 (Dense)              (None, 256)          65792       fc_value_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 6)            1542        fc_4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_4[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 653,063\n",
      "Trainable params: 653,063\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Variables\n",
      "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(500, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(500, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_3/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_3/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_3/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_3/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_4/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_4/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_4/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_4/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(256, 6) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/bias:0' shape=(6,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/kernel:0' shape=(256, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
      "Value function\n",
      "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _metrics_export_port=63419)\n",
    "checkpoint_root = '/tmp/ppo/taxi_7'\n",
    "env = 'Taxi-v3'\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['model']['fcnet_hiddens'] = [256,256,256,256]\n",
    "n_iter = 30\n",
    "full_train(checkpoint_root, env, config, n_iter)\n",
    "ray.timeline(\"/mnt/c/Users/javig/timelines/time_taxi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Midiendo tiempos específicos en el rollout\n",
    "Como hemos visto antes, el script de rollout no nos aporta información acerca de los tiempos específicos. Para ellos podemos acceder al código fuente y modificar esto manualmente.\n",
    "\n",
    "Indicamos en el código donde hemos añadido el temporizador y guardamos en un array los distintos tiempos por episodio. Vamos a modificar el script `rollout.py` y ejecutarlo directamente con los parámetros deseados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(agent,\n",
    "            env_name,\n",
    "            num_steps,\n",
    "            num_episodes=0,\n",
    "            saver=None,\n",
    "            no_render=True,\n",
    "            video_dir=None):\n",
    "    policy_agent_mapping = default_policy_agent_mapping\n",
    "\n",
    "    if saver is None:\n",
    "        saver = RolloutSaver()\n",
    "\n",
    "    if hasattr(agent, \"workers\") and isinstance(agent.workers, WorkerSet):\n",
    "        env = agent.workers.local_worker().env\n",
    "        multiagent = isinstance(env, MultiAgentEnv)\n",
    "        if agent.workers.local_worker().multiagent:\n",
    "            policy_agent_mapping = agent.config[\"multiagent\"][\n",
    "                \"policy_mapping_fn\"]\n",
    "\n",
    "        policy_map = agent.workers.local_worker().policy_map\n",
    "        state_init = {p: m.get_initial_state() for p, m in policy_map.items()}\n",
    "        use_lstm = {p: len(s) > 0 for p, s in state_init.items()}\n",
    "    else:\n",
    "        env = gym.make(env_name)\n",
    "        multiagent = False\n",
    "        try:\n",
    "            policy_map = {DEFAULT_POLICY_ID: agent.policy}\n",
    "        except AttributeError:\n",
    "            raise AttributeError(\n",
    "                \"Agent ({}) does not have a `policy` property! This is needed \"\n",
    "                \"for performing (trained) agent rollouts.\".format(agent))\n",
    "        use_lstm = {DEFAULT_POLICY_ID: False}\n",
    "\n",
    "    action_init = {\n",
    "        p: flatten_to_single_ndarray(m.action_space.sample())\n",
    "        for p, m in policy_map.items()\n",
    "    }\n",
    "\n",
    "    # If monitoring has been requested, manually wrap our environment with a\n",
    "    # gym monitor, which is set to record every episode.\n",
    "    if video_dir:\n",
    "        env = gym_wrappers.Monitor(\n",
    "            env=env,\n",
    "            directory=video_dir,\n",
    "            video_callable=lambda x: True,\n",
    "            force=True)\n",
    "\n",
    "    steps = 0\n",
    "    episodes = 0\n",
    "    times = []\n",
    "    while keep_going(steps, num_steps, episodes, num_episodes): # Este bucle es para el número de episodio o pasos\n",
    "        mapping_cache = {}  # in case policy_agent_mapping is stochastic\n",
    "        saver.begin_rollout()\n",
    "        obs = env.reset()\n",
    "        agent_states = DefaultMapping(\n",
    "            lambda agent_id: state_init[mapping_cache[agent_id]])\n",
    "        prev_actions = DefaultMapping(\n",
    "            lambda agent_id: action_init[mapping_cache[agent_id]])\n",
    "        prev_rewards = collections.defaultdict(lambda: 0.)\n",
    "        done = False\n",
    "        reward_total = 0.0\n",
    "        episode_time = 0.0\n",
    "        while not done and keep_going(steps, num_steps, episodes,\n",
    "                                      num_episodes):  #este bucle es para cada episodio\n",
    "            multi_obs = obs if multiagent else {_DUMMY_AGENT_ID: obs}\n",
    "            action_dict = {}\n",
    "            for agent_id, a_obs in multi_obs.items(): \n",
    "                if a_obs is not None:\n",
    "                    policy_id = mapping_cache.setdefault(\n",
    "                        agent_id, policy_agent_mapping(agent_id))\n",
    "                    p_use_lstm = use_lstm[policy_id]\n",
    "                    # Aquí es donde empezamos a aplicar el modelo para ver que acción tomar.\n",
    "                    \n",
    "                    ################\n",
    "                    t0 = time.time()\n",
    "                    ################\n",
    "                    \n",
    "                    if p_use_lstm:\n",
    "                        a_action, p_state, _ = agent.compute_action(\n",
    "                            a_obs,\n",
    "                            state=agent_states[agent_id],\n",
    "                            prev_action=prev_actions[agent_id],\n",
    "                            prev_reward=prev_rewards[agent_id],\n",
    "                            policy_id=policy_id)\n",
    "                        agent_states[agent_id] = p_state\n",
    "                    else:\n",
    "                        a_action = agent.compute_action(\n",
    "                            a_obs,\n",
    "                            prev_action=prev_actions[agent_id],\n",
    "                            prev_reward=prev_rewards[agent_id],\n",
    "                            policy_id=policy_id)\n",
    "                        \n",
    "                    ########################\n",
    "                    t1 = time.time()\n",
    "                    episode_time += (t1-t0)\n",
    "                    ########################\n",
    "                    \n",
    "                    a_action = flatten_to_single_ndarray(a_action)\n",
    "                    action_dict[agent_id] = a_action\n",
    "                    prev_actions[agent_id] = a_action\n",
    "            action = action_dict\n",
    "\n",
    "            action = action if multiagent else action[_DUMMY_AGENT_ID]\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            if multiagent:\n",
    "                for agent_id, r in reward.items():\n",
    "                    prev_rewards[agent_id] = r\n",
    "            else:\n",
    "                prev_rewards[_DUMMY_AGENT_ID] = reward\n",
    "\n",
    "            if multiagent:\n",
    "                done = done[\"__all__\"]\n",
    "                reward_total += sum(reward.values())\n",
    "            else:\n",
    "                reward_total += reward\n",
    "            if not no_render:\n",
    "                env.render()\n",
    "            saver.append_step(obs, action, next_obs, reward, done, info)\n",
    "            steps += 1\n",
    "            obs = next_obs\n",
    "        saver.end_rollout()\n",
    "        print(\"Episode #{}: reward: {}\".format(episodes, reward_total))\n",
    "        \n",
    "        ####################################################################\n",
    "        print(\"Episode #{}: model_time: {}\".format(episodes, episode_time))\n",
    "        times.append[episode_time]\n",
    "        ####################################################################\n",
    "        \n",
    "        if done:\n",
    "            episodes += 1\n",
    "            \n",
    "    ########################\n",
    "    print(\"Episodes times:\")\n",
    "    print(times)\n",
    "    ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "t0=time.time()\n",
    "!python3 rollout.py /tmp/ppo/taxi/checkpoint_30/checkpoint-30 --env=Taxi-v3 --run PPO --episodes 10 --out='taxi_time.pkl' --save-info --use-shelve\n",
    "t1 = time.time()-t0\n",
    "print(\"Rollout total time: \" + str(t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juegos basados en imágenes: categoría Atari\n",
    "\n",
    "Estos juegos aprenden a partir de imágenes de videojuegos. Hemos elgido como ejemplo el entorno `Pong-v0`.\n",
    "La red neuronal que se crea es una VisionNet de TF, definida para RLLIB y que cuenta con tantas como filtros de convolución se especifiquen en la configuración.\n",
    "\n",
    "Las observaciones tienen un tamaño de (210,160,3), que se preprocesa para convertirla en una de la forma (84,84,4), dimesiones que sí son compatibles con los modelos preconfigurados de tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-20 01:59:16,010\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=11851)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=11851)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=11851)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=11853)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=11853)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=11853)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-20 01:59:27,246\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': [[16, [8, 8], 4], [32, [4, 4], 2], [256, [11, 11], 1]], 'conv_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, '_time_major': False, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None}\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 84, 84, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_1 (Conv2D)           (None, 21, 21, 16)   4112        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 21, 21, 16)   4112        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_2 (Conv2D)           (None, 11, 11, 32)   8224        conv_value_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 11, 11, 32)   8224        conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_3 (Conv2D)           (None, 1, 1, 256)    991488      conv_value_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 1, 1, 256)    991488      conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_out (Conv2D)         (None, 1, 1, 1)      257         conv_value_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_out (Conv2D)               (None, 1, 1, 6)      1542        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           conv_value_out[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,009,447\n",
      "Trainable params: 2,009,447\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "agent = ppo.PPOTrainer(config, env='Pong-v0')\n",
    "policy=agent.get_policy()\n",
    "print(policy.model.model_config)\n",
    "print(policy.model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspeccionando el código de `visionnet.py` encontramos que se nos crea, por defecto, una red con tres capas de convolución con los parámetros especificados.\n",
    "- Tamaño del espacio de salida (número de filtros en la convolución)\n",
    "- Tamaño de la ventana de convolución\n",
    "- Strides: desplazamiento de la ventana por ancho y por largo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ejecutamos un entrenamiento de 5 episodios (tarda casi una hora en completarse) con el entorno `Pong-v0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 00:25:17,174\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=3859)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3859)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3859)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=3857)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3857)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3857)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-19 00:25:30,829\tINFO trainable.py:252 -- Trainable.setup took 10.082 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-11-19 00:25:30,830\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=3857)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3857)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3857)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=3859)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3859)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3859)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward    nan/   nan/   nan len    nan learn_time(ms) 453236.95 saved /tmp/ppo/pong/checkpoint_1/checkpoint-1\n",
      "  2 reward -21.00/-21.00/-21.00 len 1021.25 learn_time(ms) 444702.60 saved /tmp/ppo/pong/checkpoint_2/checkpoint-2\n",
      "  3 reward -21.00/-21.00/-21.00 len 1021.50 learn_time(ms) 441895.95 saved /tmp/ppo/pong/checkpoint_3/checkpoint-3\n",
      "  4 reward -21.00/-21.00/-21.00 len 1024.75 learn_time(ms) 437422.36 saved /tmp/ppo/pong/checkpoint_4/checkpoint-4\n",
      "  5 reward -21.00/-21.00/-21.00 len 1023.06 learn_time(ms) 442213.97 saved /tmp/ppo/pong/checkpoint_5/checkpoint-5\n",
      "Total learn time: 2219471.838\n",
      "Average learn time per iteration: 73982.3946\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 84, 84, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_1 (Conv2D)           (None, 21, 21, 16)   4112        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 21, 21, 16)   4112        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_2 (Conv2D)           (None, 11, 11, 32)   8224        conv_value_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 11, 11, 32)   8224        conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_3 (Conv2D)           (None, 1, 1, 256)    991488      conv_value_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 1, 1, 256)    991488      conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_out (Conv2D)         (None, 1, 1, 1)      257         conv_value_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_out (Conv2D)               (None, 1, 1, 6)      1542        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           conv_value_out[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,009,447\n",
      "Trainable params: 2,009,447\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Variables\n",
      "[<tf.Variable 'default_policy/conv_value_1/kernel:0' shape=(8, 8, 4, 16) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv_value_1/bias:0' shape=(16,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv1/kernel:0' shape=(8, 8, 4, 16) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv1/bias:0' shape=(16,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv_value_2/kernel:0' shape=(4, 4, 16, 32) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv_value_2/bias:0' shape=(32,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv2/kernel:0' shape=(4, 4, 16, 32) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv2/bias:0' shape=(32,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv_value_3/kernel:0' shape=(11, 11, 32, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv_value_3/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv3/kernel:0' shape=(11, 11, 32, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv3/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv_value_out/kernel:0' shape=(1, 1, 256, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv_value_out/bias:0' shape=(1,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv_out/kernel:0' shape=(1, 1, 256, 6) dtype=float32>,\n",
      " <tf.Variable 'default_policy/conv_out/bias:0' shape=(6,) dtype=float32>]\n",
      "Value function\n",
      "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _metrics_export_port=63419)\n",
    "checkpoint_root = '/tmp/ppo/pong'\n",
    "env = 'Pong-v0'\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "n_iter = 5\n",
    "full_train(checkpoint_root, env, config, n_iter)\n",
    "#ray.timeline(\"/mnt/c/Users/javig/timelines/time_taxi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos rollout con el comando de consola, sin poder medir los tiempos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2020-12-02 00:50:15,913\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2020-12-02 00:50:18,577\tINFO trainer.py:592 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-12-02 00:50:18,577\tINFO trainer.py:1064 -- `_use_trajectory_view_api` only supported for PyTorch so far! Will run w/o.\n",
      "2020-12-02 00:50:18,577\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1702)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=1702)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=1702)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=1706)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=1706)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=1706)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-12-02 00:50:26,542\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "2020-12-02 00:50:26,754\tINFO trainable.py:481 -- Restored on 10.10.1.128 from checkpoint: /tmp/ppo/pong/checkpoint_5/checkpoint-5\n",
      "2020-12-02 00:50:26,754\tINFO trainable.py:489 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 2260.505802631378, '_episodes_total': 16}\n",
      "Episode #0: reward: 250.0\n",
      "Episode #1: reward: 400.0\n",
      "Episode #2: reward: 525.0\n",
      "Episode #3: reward: 800.0\n",
      "Episode #4: reward: 100.0\n",
      "Episode #5: reward: 500.0\n",
      "Episode #6: reward: 525.0\n",
      "Episode #7: reward: 150.0\n",
      "Episode #8: reward: 275.0\n",
      "Episode #9: reward: 150.0\n",
      "Rollout time: 49.448962926864624\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "t0=time.time()\n",
    "!rllib rollout /tmp/ppo/pong/checkpoint_5/checkpoint-5 --env=AirRaid-v0 --run PPO --episodes=10 --out='airRaid.pkl' --save-info --use-shelve --no-render\n",
    "t1 = time.time()-t0\n",
    "print(\"Rollout time: \" + str(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shelve.open('airRaid.pkl') as rollouts:\n",
    "    for episode_index in range(rollouts[\"num_episodes\"]):\n",
    "        rollout = rollouts[str(episode_index)]\n",
    "        print(str(episode_index))\n",
    "        pprint.pprint(rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2020-12-02 01:33:44,618\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2020-12-02 01:33:47,311\tINFO trainer.py:592 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-12-02 01:33:47,311\tINFO trainer.py:1064 -- `_use_trajectory_view_api` only supported for PyTorch so far! Will run w/o.\n",
      "2020-12-02 01:33:47,311\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=2246)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=2246)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=2246)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=2249)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=2249)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=2249)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-12-02 01:33:55,246\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "2020-12-02 01:33:55,462\tINFO trainable.py:481 -- Restored on 10.10.1.128 from checkpoint: /tmp/ppo/pong/checkpoint_5/checkpoint-5\n",
      "2020-12-02 01:33:55,462\tINFO trainable.py:489 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 2260.505802631378, '_episodes_total': 16}\n",
      "Episode #0: reward: 175.0\n",
      "Episode #0: model_time: 1.5967423915863037\n",
      "Episode #1: reward: 300.0\n",
      "Episode #1: model_time: 1.6046392917633057\n",
      "Episode #2: reward: 1525.0\n",
      "Episode #2: model_time: 4.969146490097046\n",
      "Episode #3: reward: 400.0\n",
      "Episode #3: model_time: 1.3673248291015625\n",
      "Episode #4: reward: 0.0\n",
      "Episode #4: model_time: 0.8413338661193848\n",
      "Episode #5: reward: 200.0\n",
      "Episode #5: model_time: 0.6289956569671631\n",
      "Episode #6: reward: 325.0\n",
      "Episode #6: model_time: 0.6594774723052979\n",
      "Episode #7: reward: 25.0\n",
      "Episode #7: model_time: 0.9523022174835205\n",
      "Episode #8: reward: 50.0\n",
      "Episode #8: model_time: 1.106269359588623\n",
      "Episode #9: reward: 150.0\n",
      "Episode #9: model_time: 1.2928225994110107\n",
      "Episodes times:\n",
      "[1.5967423915863037, 1.6046392917633057, 4.969146490097046, 1.3673248291015625, 0.8413338661193848, 0.6289956569671631, 0.6594774723052979, 0.9523022174835205, 1.106269359588623, 1.2928225994110107]\n",
      "Total model time: 15.019054174423218\n",
      "Average model time per episode: 1.5019054174423219\n",
      "Rollout total time: 58.78924298286438\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "t0=time.time()\n",
    "!python3 rollout.py /tmp/ppo/pong/checkpoint_5/checkpoint-5 --env='AirRaid-v0' --run PPO --episodes 10 --out='airRaid_time.pkl' --save-info --use-shelve\n",
    "t1 = time.time()-t0\n",
    "print(\"Rollout total time: \" + str(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2020-11-20 10:45:41,326\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2020-11-20 10:45:43,955\tINFO trainer.py:592 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-11-20 10:45:43,955\tINFO trainer.py:1064 -- `_use_trajectory_view_api` only supported for PyTorch so far! Will run w/o.\n",
      "2020-11-20 10:45:43,955\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1168)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=1168)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=1168)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=1171)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=1171)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=1171)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-20 10:45:51,726\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "2020-11-20 10:45:51,945\tINFO trainable.py:481 -- Restored on 10.10.1.128 from checkpoint: /tmp/ppo/pong/checkpoint_5/checkpoint-5\n",
      "2020-11-20 10:45:51,945\tINFO trainable.py:489 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 2260.505802631378, '_episodes_total': 16}\n",
      "Episode #0: reward: 0.0\n",
      "Episode #0: model_time: 17.59477663040161\n",
      "Episode #1: reward: 0.0\n",
      "Episode #1: model_time: 18.401540756225586\n",
      "Episode #2: reward: 0.0\n",
      "Episode #2: model_time: 16.915698528289795\n",
      "Episode #3: reward: 0.0\n",
      "Episode #3: model_time: 17.301233053207397\n",
      "Episode #4: reward: 0.0\n",
      "Episode #4: model_time: 18.61392307281494\n",
      "Episode #5: reward: 0.0\n",
      "Episode #5: model_time: 18.55021619796753\n",
      "Episode #6: reward: 0.0\n",
      "Episode #6: model_time: 18.564754724502563\n",
      "Episode #7: reward: 0.0\n",
      "Episode #7: model_time: 17.620477199554443\n",
      "Episode #8: reward: 0.0\n",
      "Episode #8: model_time: 21.097277879714966\n",
      "Episode #9: reward: 0.0\n",
      "Episode #9: model_time: 17.87918210029602\n",
      "Episodes times:\n",
      "[17.59477663040161, 18.401540756225586, 16.915698528289795, 17.301233053207397, 18.61392307281494, 18.55021619796753, 18.564754724502563, 17.620477199554443, 21.097277879714966, 17.87918210029602]\n",
      "Total model time: 182.53908014297485\n",
      "Average model time per episode: 18.253908014297487\n",
      "Rollout total time: 473.0207304954529\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "t0=time.time()\n",
    "!python3 rollout.py /tmp/ppo/pong/checkpoint_5/checkpoint-5 --env='Bowling-v0' --run PPO --episodes 10 --out='bowling.pkl' --save-info --use-shelve\n",
    "t1 = time.time()-t0\n",
    "print(\"Rollout total time: \" + str(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2020-12-02 00:39:39,824\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2020-12-02 00:39:42,521\tINFO trainer.py:592 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-12-02 00:39:42,521\tINFO trainer.py:1064 -- `_use_trajectory_view_api` only supported for PyTorch so far! Will run w/o.\n",
      "2020-12-02 00:39:42,521\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=602)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=602)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=602)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=600)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=600)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=600)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-12-02 00:39:50,152\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "2020-12-02 00:39:50,372\tINFO trainable.py:481 -- Restored on 10.10.1.128 from checkpoint: /tmp/ppo/pong/checkpoint_5/checkpoint-5\n",
      "2020-12-02 00:39:50,372\tINFO trainable.py:489 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 2260.505802631378, '_episodes_total': 16}\n",
      "Traceback (most recent call last):\n",
      "  File \"rollout.py\", line 505, in <module>\n",
      "    run(args, parser)\n",
      "  File \"rollout.py\", line 313, in run\n",
      "    rollout(agent, args.env, num_steps, num_episodes, saver,\n",
      "  File \"rollout.py\", line 458, in rollout\n",
      "    env.render()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/core.py\", line 240, in render\n",
      "    return self.env.render(mode, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/core.py\", line 240, in render\n",
      "    return self.env.render(mode, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/core.py\", line 240, in render\n",
      "    return self.env.render(mode, **kwargs)\n",
      "  [Previous line repeated 4 more times]\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/envs/atari/atari_env.py\", line 152, in render\n",
      "    from gym.envs.classic_control import rendering\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/envs/classic_control/rendering.py\", line 25, in <module>\n",
      "    from pyglet.gl import *\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyglet/gl/__init__.py\", line 244, in <module>\n",
      "    import pyglet.window\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyglet/window/__init__.py\", line 1880, in <module>\n",
      "    gl._create_shadow_window()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyglet/gl/__init__.py\", line 220, in _create_shadow_window\n",
      "    _shadow_window = Window(width=1, height=1, visible=False)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyglet/window/xlib/__init__.py\", line 165, in __init__\n",
      "    super(XlibWindow, self).__init__(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyglet/window/__init__.py\", line 570, in __init__\n",
      "    display = pyglet.canvas.get_display()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyglet/canvas/__init__.py\", line 94, in get_display\n",
      "    return Display()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyglet/canvas/xlib.py\", line 123, in __init__\n",
      "    raise NoSuchDisplayException('Cannot connect to \"%s\"' % name)\n",
      "pyglet.canvas.xlib.NoSuchDisplayException: Cannot connect to \"None\"\n",
      "Rollout total time: 19.30210828781128\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "t0=time.time()\n",
    "!python3 rollout.py /tmp/ppo/pong/checkpoint_5/checkpoint-5 --env='Carnival-v0' --run PPO --episodes 10 --out='carnival.pkl' --save-info --use-shelve\n",
    "t1 = time.time()-t0\n",
    "print(\"Rollout total time: \" + str(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2020-11-20 10:59:10,489\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2020-11-20 10:59:12,753\tINFO trainer.py:592 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-11-20 10:59:12,753\tINFO trainer.py:1064 -- `_use_trajectory_view_api` only supported for PyTorch so far! Will run w/o.\n",
      "2020-11-20 10:59:12,753\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1706)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=1706)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=1706)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=1704)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=1704)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=1704)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-20 10:59:20,559\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n",
      "2020-11-20 10:59:20,771\tINFO trainable.py:481 -- Restored on 10.10.1.128 from checkpoint: /tmp/ppo/pong/checkpoint_5/checkpoint-5\n",
      "2020-11-20 10:59:20,771\tINFO trainable.py:489 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 2260.505802631378, '_episodes_total': 16}\n",
      "Episode #0: reward: 10.0\n",
      "Episode #0: model_time: 3.22743821144104\n",
      "Episode #1: reward: 10.0\n",
      "Episode #1: model_time: 2.166896104812622\n",
      "Episode #2: reward: 0.0\n",
      "Episode #2: model_time: 0.0887136459350586\n",
      "Episode #3: reward: 70.0\n",
      "Episode #3: model_time: 12.133976221084595\n",
      "Episode #4: reward: 0.0\n",
      "Episode #4: model_time: 0.18700194358825684\n",
      "Episode #5: reward: 30.0\n",
      "Episode #5: model_time: 17.372771978378296\n",
      "Episode #6: reward: 10.0\n",
      "Episode #6: model_time: 1.1136856079101562\n",
      "Episode #7: reward: 0.0\n",
      "Episode #7: model_time: 0.07588052749633789\n",
      "Episode #8: reward: 0.0\n",
      "Episode #8: model_time: 0.0676429271697998\n",
      "Episode #9: reward: 10.0\n",
      "Episode #9: model_time: 0.08051419258117676\n",
      "Episodes times:\n",
      "[3.22743821144104, 2.166896104812622, 0.0887136459350586, 12.133976221084595, 0.18700194358825684, 17.372771978378296, 1.1136856079101562, 0.07588052749633789, 0.0676429271697998, 0.08051419258117676]\n",
      "Total model time: 36.51452136039734\n",
      "Average model time per episode: 3.651452136039734\n",
      "Rollout total time: 117.71482706069946\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "t0=time.time()\n",
    "!python3 rollout.py /tmp/ppo/pong/checkpoint_5/checkpoint-5 --env='DemonAttack-v0' --run PPO --episodes 10 --out='demonattack.pkl' --save-info --use-shelve\n",
    "t1 = time.time()-t0\n",
    "print(\"Rollout total time: \" + str(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"rollout.py\", line 466\n",
      "    average_time = episode_time/steps\n",
      "                                    ^\n",
      "TabError: inconsistent use of tabs and spaces in indentation\n",
      "Rollout total time: 0.46683597564697266\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "t0=time.time()\n",
    "!python3 rollout.py /tmp/ppo/pong/checkpoint_5/checkpoint-5 --env='Pong-v0' --run PPO --episodes 10 --out='pong.pkl' --save-info --use-shelve\n",
    "t1 = time.time()-t0\n",
    "print(\"Rollout total time: \" + str(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-20 12:08:11,766\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=3812)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3812)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3812)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=3815)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3815)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3815)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-11-20 12:08:21,693\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "agent = ppo.PPOTrainer(config, env='Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.policy.tf_policy_template.PPOTFPolicy at 0x7f426404c430>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, '_time_major': False, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None}\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          33024       observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          33024       observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 6)            1542        fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 199,431\n",
      "Trainable params: 199,431\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_last_output', '_value_out', 'action_space', 'base_model', 'context', 'custom_loss', 'forward', 'framework', 'from_batch', 'get_initial_state', 'graph', 'import_from_h5', 'inference_view_requirements', 'is_time_major', 'last_output', 'metrics', 'model_config', 'name', 'num_outputs', 'obs_space', 'register_variables', 'time_major', 'trainable_variables', 'update_ops', 'value_function', 'var_list', 'variables']\n",
      "{'obs': <ray.rllib.policy.view_requirement.ViewRequirement object at 0x7f426404c520>}\n"
     ]
    }
   ],
   "source": [
    "policy=agent.get_policy()\n",
    "print(policy.model.model_config)\n",
    "print(policy.model.base_model.summary())\n",
    "print(dir(policy.model))\n",
    "print(policy.model.inference_view_requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1255..1573 -> 318-tiles track\n",
      "(96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v0\")\n",
    "print(env.reset().shape)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.models.preprocessors.GenericPixelPreprocessor at 0x7f41bfee7fd0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "prep = get_preprocessor(env.observation_space)(env.observation_space)\n",
    "prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.transform(env.reset()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "print(type(env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.reset()[25:-25, :, :])\n",
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ae18def96b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# take a random action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-03 19:32:13,564\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2020-12-03 19:32:22,119\tINFO trainer.py:592 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-12-03 19:32:22,121\tINFO trainer.py:1064 -- `_use_trajectory_view_api` only supported for PyTorch so far! Will run w/o.\n",
      "2020-12-03 19:32:22,122\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=275)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=275)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=275)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=276)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=276)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=276)\u001b[0m non-resource variables are not supported in the long term\n",
      "2020-12-03 19:32:36,683\tINFO trainable.py:252 -- Trainable.setup took 14.564 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-12-03 19:32:36,684\tWARNING util.py:40 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _metrics_export_port=63419)\n",
    "#checkpoint_root = '/tmp/ppo/taxi_3'\n",
    "env = 'Pong-v0'\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['model']['dim'] = 168\n",
    "config['model']['conv_filters'] = [[16, [16, 16], 8],[32, [8, 8], 2],[256, [11, 11], 1]]\n",
    "agent = ppo.PPOTrainer(config, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 168, 168, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_1 (Conv2D)           (None, 21, 21, 16)   16400       observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 21, 21, 16)   16400       observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_2 (Conv2D)           (None, 11, 11, 32)   32800       conv_value_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 11, 11, 32)   32800       conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_3 (Conv2D)           (None, 1, 1, 256)    991488      conv_value_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 1, 1, 256)    991488      conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_out (Conv2D)         (None, 1, 1, 1)      257         conv_value_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_out (Conv2D)               (None, 1, 1, 6)      1542        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           conv_value_out[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,083,175\n",
      "Trainable params: 2,083,175\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(agent.get_policy().model.base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _metrics_export_port=63419)\n",
    "#checkpoint_root = '/tmp/ppo/taxi_3'\n",
    "env = 'Pong-v0'\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "#config['model']['dim'] = 84\n",
    "#config['preprocessor_pref'] = 'deepmind'\n",
    "agent = ppo.PPOTrainer(config, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 168, 168, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_1 (Conv2D)           (None, 21, 21, 16)   16400       observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 21, 21, 16)   16400       observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_2 (Conv2D)           (None, 11, 11, 32)   32800       conv_value_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 11, 11, 32)   32800       conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_3 (Conv2D)           (None, 1, 1, 256)    991488      conv_value_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 1, 1, 256)    991488      conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_out (Conv2D)         (None, 1, 1, 1)      257         conv_value_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_out (Conv2D)               (None, 1, 1, 6)      1542        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           conv_value_out[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,083,175\n",
      "Trainable params: 2,083,175\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m F1203 23:49:26.063741  1432  1432 redis_client.cc:74]  Check failed: num_attempts < RayConfig::instance().redis_db_connect_retries() Expected 1 Redis shard addresses, found 2\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m *** Check failure stack trace: ***\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47f0b4cd  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47f0c93c  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47f0b1a9  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47f0b3c1  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47ecff39  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47e00773  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47e01291  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47da00ba  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47cb2da1  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47c56a8c  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47c201ac  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb475970b3  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fbb47c2e4e1  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m F1203 23:49:26.567319  1446  1446 redis_client.cc:74]  Check failed: num_attempts < RayConfig::instance().redis_db_connect_retries() Expected 1 Redis shard addresses, found 2\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m *** Check failure stack trace: ***\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c64ded3d  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c64e01ac  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c64dea19  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c64dec31  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c64932b9  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c6308053  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c6308b71  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c62a947a  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c6242456  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c60f65b3  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c5a570b3  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f50c6109621  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m F1203 23:49:51.779527  1482  1482 redis_client.cc:74]  Check failed: num_attempts < RayConfig::instance().redis_db_connect_retries() Expected 1 Redis shard addresses, found 3\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m *** Check failure stack trace: ***\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4ecb24cd  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4ecb393c  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4ecb21a9  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4ecb23c1  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4ec76f39  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4eba7773  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4eba8291  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4eb470ba  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4ea59da1  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4e9fda8c  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4e9c71ac  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4e3470b3  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m     @     0x7fde4e9d54e1  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m F1203 23:49:52.287292  1496  1496 redis_client.cc:74]  Check failed: num_attempts < RayConfig::instance().redis_db_connect_retries() Expected 1 Redis shard addresses, found 3\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m *** Check failure stack trace: ***\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f5319940d3d  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f53199421ac  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f5319940a19  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f5319940c31  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f53198f52b9  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f531976a053  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f531976ab71  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f531970b47a  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f53196a4456  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f53195585b3  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f5318eb70b3  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f531956b621  (unknown)\n"
     ]
    }
   ],
   "source": [
    "print(agent.get_policy().model.base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

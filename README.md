# Evaluación de rendimiento de arquitecturas paralelas y de propósito específico para el aprendizaje por refuerzo en juegos
## Trabajo de Fin de Grado
### Javier Guzmán Muñoz
### Doble Grado en Ingeniería Informática y Matemáticas 
### Universidad Complutense de Madrid

Las aplicaciones de aprendizaje por refuerzo se usan en la actualidad para resolver problemas de todo tipo en campos muy diversos. Sin embargo, una de las principales desventajas que presentan es el elevado coste computacional del entrenamiento de los modelos necesarios. Con este trabajo de fin de grado se pretende mejorar este proceso mediante la paralelización de los algoritmos empleados y el uso de distintas arquitecturas hardware que variarán los tiempos empleados. Los modelos entrenados pueden aplicarse para obtener la mejor secuencia de acciones que podemos realizar sobre un entorno y que mejore la recompensa obtenida. Este proceso, que se denomina inferencia, es ya de por sí bastante eficiente en tiempo, pero la existencia de procesadores de propósito específico para realizar esta tarea hace también conveniente evaluar su rendimiento en estos soportes y compararlos con otras unidades de procesamiento más generales. Tras definir en el escenario en el que nos vamos a mover y los recursos necesarios para ello, se definen una serie de experimentos de los procesos de entrenamiento e inferencia que nos permitirán evaluar el rendimiento en términos del tiempo empleado, de la utilización de los recursos disponibles y del consumo de energía de distintas arquitecturas hardware, viendo cuál es más conveniente usar en cada caso.

El presente repositorio contiene el código necesario para llevar a cabo diferentes experimentos de entrenamiento e inferencia con una configuración especificada. Los entrenamientos se realizan dentro del framework de Ray, concretamente de su biblioteca de aprendizaje por refuerzo [RLlib](https://docs.ray.io/en/master/rllib.html) y usando el algoritmo PPO, que permite una paralelización de sus etapas para mejorar el rendimiento. Los procesos de inferencia, esto es, la aplicación de modelos previamente entrenados para obtener la mejor secuencia de acciones que maximicen la recompensa final, pueden llevarse a cabo usando la funcionalidad de RLlib. Adaptamos los scripts propios de la biblioteca para nuestro propósito, añadiendo funcionalidad que nos reporte información acerca del tiempo empleado. Además, ofrecemos los scripts necesario para llevar a cabo la transformación de un modelo entrenado con RLlib hata uno de Tensorflow Lite compilado para poder ser ejecutado en el acalerador Google Coral que incluye una TPU, un procesador de propósito específico para llevar a cabo inferencias de modelos de aprendizaje profundo. 

La memoria del proyecto contiene información detallada acerca de todo el proceso de experimentación que se lleva a cabo y de los resultados obtenidos. Además, se incluye una guía de uso de los principales scripts que componen este repositorio. Hay que tener en cuenta que los experimentos se realizan en un servidor que, entre otros recursos, cuenta con dos GPUs y los scripts están adaptados para este propósito.


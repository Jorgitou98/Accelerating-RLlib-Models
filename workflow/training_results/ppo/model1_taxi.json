["{\"n\": 1, \"episode_reward_min\": -893.0, \"episode_reward_mean\": -770.6, \"episode_reward_max\": -668.0, \"episode_len_mean\": 200.0, \"learn_time_ms\": 2571.318, \"total_train_time_s\": 6.463725328445435}", "{\"n\": 2, \"episode_reward_min\": -893.0, \"episode_reward_mean\": -751.175, \"episode_reward_max\": -287.0, \"episode_len_mean\": 197.3, \"learn_time_ms\": 3514.633, \"total_train_time_s\": 8.330763816833496}", "{\"n\": 3, \"episode_reward_min\": -893.0, \"episode_reward_mean\": -729.8333333333334, \"episode_reward_max\": -287.0, \"episode_len_mean\": 196.68333333333334, \"learn_time_ms\": 3570.695, \"total_train_time_s\": 7.503092050552368}", "{\"n\": 4, \"episode_reward_min\": -893.0, \"episode_reward_mean\": -717.525, \"episode_reward_max\": -287.0, \"episode_len_mean\": 197.5125, \"learn_time_ms\": 3542.547, \"total_train_time_s\": 7.416043043136597}", "{\"n\": 5, \"episode_reward_min\": -893.0, \"episode_reward_mean\": -697.66, \"episode_reward_max\": -206.0, \"episode_len_mean\": 196.48, \"learn_time_ms\": 3879.625, \"total_train_time_s\": 9.112382650375366}", "{\"n\": 6, \"episode_reward_min\": -884.0, \"episode_reward_mean\": -663.46, \"episode_reward_max\": -206.0, \"episode_len_mean\": 196.48, \"learn_time_ms\": 4067.223, \"total_train_time_s\": 8.823630094528198}", "{\"n\": 7, \"episode_reward_min\": -884.0, \"episode_reward_mean\": -622.04, \"episode_reward_max\": -206.0, \"episode_len_mean\": 196.37, \"learn_time_ms\": 4027.406, \"total_train_time_s\": 7.619020462036133}", "{\"n\": 8, \"episode_reward_min\": -965.0, \"episode_reward_mean\": -590.86, \"episode_reward_max\": -206.0, \"episode_len_mean\": 197.23, \"learn_time_ms\": 4138.591, \"total_train_time_s\": 8.781495571136475}", "{\"n\": 9, \"episode_reward_min\": -965.0, \"episode_reward_mean\": -545.81, \"episode_reward_max\": -206.0, \"episode_len_mean\": 196.07, \"learn_time_ms\": 4149.898, \"total_train_time_s\": 8.232675313949585}", "{\"n\": 10, \"episode_reward_min\": -965.0, \"episode_reward_mean\": -496.33, \"episode_reward_max\": -114.0, \"episode_len_mean\": 196.0, \"learn_time_ms\": 3997.159, \"total_train_time_s\": 6.618338346481323}"]["{\"n\": 1, \"episode_reward_min\": -902.0, \"episode_reward_mean\": -800.3, \"episode_reward_max\": -713.0, \"episode_len_mean\": 200.0, \"learn_time_ms\": 5489.483, \"total_train_time_s\": 9.401346445083618}", "{\"n\": 2, \"episode_reward_min\": -902.0, \"episode_reward_mean\": -779.6, \"episode_reward_max\": -641.0, \"episode_len_mean\": 200.0, \"learn_time_ms\": 5183.103, \"total_train_time_s\": 8.767105102539062}", "{\"n\": 3, \"episode_reward_min\": -902.0, \"episode_reward_mean\": -756.05, \"episode_reward_max\": -569.0, \"episode_len_mean\": 200.0, \"learn_time_ms\": 4939.956, \"total_train_time_s\": 8.426772832870483}", "{\"n\": 4, \"episode_reward_min\": -902.0, \"episode_reward_mean\": -730.5625, \"episode_reward_max\": -542.0, \"episode_len_mean\": 199.9375, \"learn_time_ms\": 4662.919, \"total_train_time_s\": 7.800619602203369}", "{\"n\": 5, \"episode_reward_min\": -902.0, \"episode_reward_mean\": -700.86, \"episode_reward_max\": -461.0, \"episode_len_mean\": 199.95, \"learn_time_ms\": 4628.082, \"total_train_time_s\": 8.435993194580078}", "{\"n\": 6, \"episode_reward_min\": -866.0, \"episode_reward_mean\": -659.01, \"episode_reward_max\": -452.0, \"episode_len_mean\": 199.95, \"learn_time_ms\": 4735.463, \"total_train_time_s\": 9.532742977142334}", "{\"n\": 7, \"episode_reward_min\": -866.0, \"episode_reward_mean\": -623.9, \"episode_reward_max\": -190.0, \"episode_len_mean\": 198.62, \"learn_time_ms\": 4738.852, \"total_train_time_s\": 9.764745950698853}", "{\"n\": 8, \"episode_reward_min\": -839.0, \"episode_reward_mean\": -577.72, \"episode_reward_max\": -73.0, \"episode_len_mean\": 197.11, \"learn_time_ms\": 4624.351, \"total_train_time_s\": 7.843708515167236}", "{\"n\": 9, \"episode_reward_min\": -839.0, \"episode_reward_mean\": -545.39, \"episode_reward_max\": -73.0, \"episode_len_mean\": 196.46, \"learn_time_ms\": 4551.049, \"total_train_time_s\": 8.202889204025269}", "{\"n\": 10, \"episode_reward_min\": -839.0, \"episode_reward_mean\": -518.81, \"episode_reward_max\": -73.0, \"episode_len_mean\": 196.19, \"learn_time_ms\": 4490.21, \"total_train_time_s\": 7.878126621246338}"]